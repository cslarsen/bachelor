\section{Paxos pseudo--code}

Algorithm \ref{algorithm:paxos.full.proposer} is for the Paxos proposer
role.  \texttt{TRUST}--messages are received during phase 1a, and
\texttt{PROMISE}s are received during phase 2a \cite{Lam01}.
The algorithms are a slight variation of the ones given in
\cite{Insane.Paxos}.

\begin{algorithm}
  \caption{Classic crash Paxos --- Proposer $c$ (leader)}
  \label{algorithm:paxos.full.proposer}
  \begin{algorithmic}

    \State $A$ \Comment{Set of acceptors}
    \State $crnd \gets 0$ \Comment{Current round (unique)}
    \State

    \On{$\langle \texttt{TRUST}, c \rangle$}{$\Omega_c$}
      \State $crnd \gets \textbf{pickNext}(crnd)$ \Comment{Phase 1a}
      \State $MV \gets \emptyset$ \Comment{Set of $\langle round, vote\ value \rangle$ tuples}
      \State \SendTo{$\langle \texttt{PREPARE}, crnd \rangle$}{$A$}
    \EndOn
    \State

    \On{$\langle \texttt{PROMISE}, rnd, vrnd, vval \rangle$}
       {$\text{acceptor}\ a$} \Comment{Phase 2a}
      \If{$rnd = crnd$}
        \State $MV \gets MV \cup \langle vrnd, vval \rangle$
        \If{$|MV| \geq n_a - t_a$}
          \If{$(vrnd = \bot)\ \forall\ \langle vrnd, vval \rangle \in MV$}
            \State $cval \gets \textbf{pickAny}()$
          \Else
            \State $cval \gets \textbf{pickLargest}(MV)$
          \EndIf
         \State \SendTo{$\langle \texttt{ACCEPT}, crnd, cval \rangle$}
                       {$A$}
        \EndIf
      \EndIf
    \EndOn

  \end{algorithmic}
\end{algorithm}

First is the initialization for the proposer. It has access to the set of
all acceptors $A$.  It also sets the current round number $crnd$ to
zero, but it must be a unique value per Paxos node.
Equations \ref{equation:crnd_i} and \ref{equation:crnd_mod_N} show how we
obtain a sequence of unique numbers.

Upon receiving a \texttt{TRUST} message from $\Omega_c$, it will pick the
proposal number larger than $crnd$, reset the set of
$\langle round, vote~value\rangle$ tuples and then send a
\texttt{PREPARE} message to all acceptors $A$.  Finally, it will
send a \texttt{PREPARE} message to all acceptors.

\begin{algorithm}
  \caption{Classic crash Paxos --- Acceptor $a$}
  \label{algorithm:paxos.full.acceptor}
  \begin{algorithmic}
    \State $P$ \Comment{Set of proposers}
    \State $L$ \Comment{Set of learners}
    \State $rnd \gets 0$ \Comment{Highest round seen}
    \State $vrnd \gets \bot$ \Comment{Round in which value was last accepted}
    \State $vval \gets \bot$ \Comment{Value last accepted}
    \State

    \On{$\langle \texttt{PREPARE}, n \rangle$}
       {$\text{proposer}\ c$} \Comment{Phase 1b}
      \If{$n > rnd$}
         \State $rnd \gets n$
         \State \SendTo{$\langle \texttt{PROMISE}, rnd, vrnd, vval\rangle$}
                       {$c$}
      \EndIf
    \EndOn
    \State

    \On{$\langle \texttt{ACCEPT}, n, v \rangle$}
       {$\text{proposer}\ c$} \Comment{Phase 2b}
      \If{$n \geq rnd \wedge n \neq vrnd$}
        \State $rnd \gets n$
        \State $vrnd \gets n$
        \State $vval \gets v$
        \State \SendTo{$\langle \texttt{LEARN}, n, v \rangle$}
                      {$L$}
      \EndIf
    \EndOn
  \end{algorithmic}
\end{algorithm}


\section{Simplifying the Paxos implementation}

In this chapter we will look at a simplified implementation of Paxos as
given in algorithms \ref{algorithm:paxos.full.proposer} and
\ref{algorithm:paxos.full.acceptor}.

It has been simplified to serve our needs, i.e.~to be able to handle
\texttt{ACCEPT} and \texttt{LEARN}--messages.  We can therefore remove the
$vrnd$ altogether.  Also, each Paxos node in our system will take on all
three roles, so we don't need separate sets for the acceptors, learners and
proposers. We will instead use simply $N$ for the set of Paxos nodes.

We need $crnd$ to be a sequence of unique values per Paxos node.
Instead of initializing it to zero, we will set it to the node's
unique ID.  Then, in $\textbf{pickNext}$, we will simply increment $crnd$
with the total number of Paxos nodes in the system.  This is a common trick
to ensure that each and every $crnd$ will be unique in the system and has
the added benefit that we can deduce the node ID by taking
$crnd\ (\bmod\ |N|)$, or taking the $crnd$ modulus the number of Paxos nodes
$|N|$ in the system:

Given
\begin{gather}
  crnd_i = \left\{
             \begin{array}{ll}
               n_{id} & \mbox{for } i = 0 \\
               crnd_{i-1} + |N| & \mbox{for } i \geq 1
             \end{array}
           \right. , n \in N
  \label{equation:crnd_i}
\end{gather}
then
\begin{gather}
  n_{id} \equiv crnd\ (\bmod\ |N|)\ \text{for}\ n \in N
  \label{equation:crnd_mod_N}
\end{gather}

where $n$ is the node and $N$ is the set of all nodes.  This leads to our
definition of $\textbf{pickNext}$ in algorithm
\ref{algorithm:paxos.simplified.pickNext}.

\begin{algorithm}
  \caption{Definition of \textbf{pickNext} based on equation \ref{equation:crnd_mod_N}}
  \label{algorithm:paxos.simplified.pickNext}
  \begin{algorithmic}
    \State $N$ \Comment{The set of all Paxos nodes}
    \State $n_{id} \gets \text{Unique Paxos node id}$
    \State $crnd \gets n_{id}$ \Comment{Replaces initialization of $crnd$ in algorithm \ref{algorithm:paxos.full.proposer}}
    \State
    \Function{$\textbf{pickNext}$}{}
      \State $\textbf{return}\ crnd + |N|$ \Comment{Unique per equation \ref{equation:crnd_mod_N}}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

As we only intend to show that we can implement \texttt{ACCEPT} and
\texttt{LEARN}, we can ignore \texttt{TRUST}, \texttt{PROMISE} and
\texttt{PREPARE}--messages.  This leaves us with a very simple algorithm.

\begin{algorithm}
  \caption{Simplified algorithm for processing \texttt{ACCEPT}--messages}
  \label{algorithm:paxos.simple.acceptor}
  \begin{algorithmic}
    \State $N$\Comment{The set of Paxos nodes}
    \State $rnd \gets 0$ \Comment{Current round number}
    \State $vval \gets \bot$ \Comment{Packet ID of last round}
    \State

    \On{$\langle \texttt{ACCEPT}, n, v \rangle$}{$leader$}
      \If{$n \geq rnd$} % \wedge n \neq vrnd$}
        \State $rnd\gets n$
        \State $vval\gets v$ \Comment{The client packet ID}
        \ForIn{$node$}{$N$}
           \State \SendTo{$\langle \texttt{LEARN}, n, v \rangle$}
                         {$node$}
        \EndForIn
      \EndIf
    \EndOn
  \end{algorithmic}
\end{algorithm}

As for handling \texttt{LEARN}--messages, we can proceed to send the last
fragment of the client packet to the end--hosts when we have received a
learn from a majority.

\begin{algorithm}
  \caption{Simplified algorithm for processing \texttt{LEARN}--messages}
  \label{algorithm:paxos.simple.learner}
  \begin{algorithmic}
    \State $H$ \Comment{The set of end--hosts connected to this switch}
    \State

    \On{$\langle \texttt{LEARN}, n, v \rangle$}{$acceptor$}
      \If{$got\_{}majority(n)$}
        \ForIn{$host$}{$H$}
          \State \SendTo{$ last\_{}fragment(v) $}{$host$}
        \EndForIn
      \EndIf
    \EndOn
  \end{algorithmic}
\end{algorithm}

\section{Implementing simplified Paxos in Forth}

We will implement algorithms \ref{algorithm:paxos.simple.acceptor} 
and \ref{algorithm:paxos.simple.learner} in a combination of OpenFlow
matches and Forth.

\subsection{Paxos message packets}

For transmitting Paxos messages between the switches, we don't need to use
the IP--protocol.  A nice trick in OpenFlow is just to use Ethernet packets
and identify them by marking the Ethernet type field with a special value.
The packet payload will then consist of consecutive 32--bit values: The type of
Paxos message (\texttt{ACCEPT} or \texttt{LEARN} and their parameters.
This will make it easy for the switches to extract the contents.
Of course, since we don't use IP, we can only exchange these packets on a
local network.  If we wanted to distribute the switches across the network,
we would have to use IP.

\begin{table}[H]
  \begin{tabular}{|l|l|l|l|l|}
    \hline Ethernet header & Ethernet type & \ldots & Paxos message type & Payload \\
    \hline \ldots & \texttt{PAXOS} & \ldots & \texttt{HELLO} & $ \langle node_{id} \rangle $ \\
    \hline \ldots & \texttt{PAXOS} & \ldots & \texttt{ACCEPT} & $ \langle n, v \rangle $ \\
    \hline \ldots & \texttt{PAXOS} & \ldots & \texttt{LEARN} & $ \langle n, v \rangle $ \\
    \hline
  \end{tabular}

  \caption{The structure of Paxos message in Ethernet packets}
  \label{table:paxos.ethernet.packet}
\end{table}
\todo{We need to add more Ethernet frame/packet headers here}

If we intended to implement full Paxos, we could simply add more message
types to the above structure.

\subsection{OpenFlow matching rules}

We need several OpenFlow matching rules for this to work.

First, when a switch gets a client request (a packet from the WAN) it needs
to add flow table entries that forwards the packet to the leader.

When the system starts up, the switches need to announce themselves to each
other and learn which ports they are on.  For this we use the
\texttt{HELLO}--message given in table \ref{table:paxos.ethernet.packet}.
In a full Paxos implementation, the system would then perform leader
election.  That is out of scope for this thesis, so we will just designate
the first switch as the leader.  We also simplify the system further by
letting each switch know ahead how many other Paxos nodes there are---and
this will be static throughout the lifetime of the system\footnote{A full
Paxos implementation would also implement functionality for keeping tabs
on the liveness of each node.  OpenFlow has some limited support of
notifying the controllers when link status changes.  Also, a
production--quality system would allow for nodes to join and leave the
system.}.

\begin{table}[H]
  \begin{tabular}{|l|}
    \hline \textbf{Flow table entry} \\
    \hline Forward client requests to leader \\
    \hline
  \end{tabular}

  \caption{OpenFlow flow table entries}
  \label{table:paxos.flowtable.entries}
\end{table}

We also need entries for matching Paxos messages and react on these.
This is done by inserting entries that match on Ethernet type
\texttt{PAXOS} and ingress port from the leader.
The action will be to go to a new entry that looks at what kind of Paxos
message we have received\footnote{An optimization trick would be to
combine the Paxos packet type identifier with the Paxos message type and put
them both in the Ethernet type field.  Then we could use existing OpenFlow
matching instead of having to extract the Paxos message type.}.

Finally, when matching on Paxos message types, we would execute the Forth
bytecode and forward packets based on the return value from the code.

\subsection{New OpenFlow actions}

\todo{List opp nye actions her}
For the system to perform well, we don't want to store client packets in the
switch or the controller.  Instead, it would be nice if we could just pass
along client packets directly down to the end--hosts.

However, this means that the hosts will process the packets before we have a
chance to perform Paxos ordering.

We propose a neat solution to this problem.  When a switch receives a client
message, it will immediately perform IP--fragmentation of the message and
send the first fragment to the hosts.  The hosts networking stack will then
buffer the packet and wait for the last fragment.

When the Paxos consensus algorithm terminates, we will send the last
fragment down to the host, which will then pass the packet up the stack to
the server program.

We still need to store fragments, but if we choose the fragmentation offset
wisely, we need only store very small fragments.

The downside to this is that we break MTU rules, and some systems may behave
strange---or not at all.  But for our purposes we think this is a good
solution.

For this we need some new OpenFlow actions for fragmenting packets, storing
them and then forwarding the stored remaining fragment.

\begin{table}[H]
  \begin{tabular}{|l|l|l|}
    \hline \textbf{Action} & \textbf{Parameters} & \textbf{Description} \\
    \hline Fragment packet & buffer id, fragment offset & ... \\
    \hline Defragment packet & buffer id, buffer id & ... \\
    \hline Store fragment in table & buffer id & ... \\
    \hline Retrieve fragment from table & buffer id & ... \\
    \hline
  \end{tabular}

  \caption{New OpenFlow actions}
  \label{table:openflow.new.actions}
\end{table}

We also need new OpenFlow protocol messages so that the controller is able
to install flows with these new actions.  However, because of the scope of
this thesis, we will simply store these actions directly in OpenVSwitch and
pretend that these actions and flow entries came from the
controller\footnote{While trivial, this takes a little work to do fully.
One would first have to modify the OpenFlow protocol with new actions,
implement them and then do the same modifications on the controller.}.

\subsection{The switch data table}

Since each Paxos node needs to remember values for the round number, number
of nodes and so on, we propose that we add a simple table to each switch.

This is done by modifying the OpenVSwitch source code.

To conserve memory, we propose that each switch gets a table with 256
entries containing 32--bit values, for a total of 1024 bytes of memory.

OpenVSwitch needs to expose internally functions for manipulating this table
to Forth.  Our Forth code can then store the node id at location 0, round
numbers at location 1 and so on.

\subsection{Paxos message handlers in Forth}

Here we will define the Forth code for the Paxos message handlers.

\begin{verbatim}
variable crnd
variable node_id
variable vval
variable |N|

: pickNext ( -- crnd + |N| )
    \ Calculate the next value of crnd
    crnd @ |N| @ + ;

: on_accept ( n v -- 0 or -1 )
    over dup      ( n v -- v n n )
    rnd @ >= if   ( v n n -- v n )
        rnd !     ( v n -- v )
        vval !    ( v -- )
        -1        \ Return value TRUE
    else
        drop drop ( v n -- )
        0         \ Return value FALSE
    then ;
\end{verbatim}

\subsection{Why Forth?}

\todo{Insert a "defense" of why we chose Forth, and why we didn't implement
everything in OpenFlow}

What we want to avoid is to implement a complete, Turing--complete
programming language on the switch.  If that was our intention, we should
simply have used a platform that already had that, like Intel's NetVM
platform or NetFPGA\todo{Finn ut om flere alternative, og sjekk opp at disse
  nevnte er aktuelle}.

Instead, what we want is to provide \textit{simple} primitives that can be
implemented to run \textit{efficiently} on the hardware---requiring little
memory and few cycles per operation---while still being useful for other
networking protocols. (back up this statement on the last part)

In fact, our solution can, in fact, be implemented in a branchless manner on the
switch CPU, meaning that one can fill up the instruction pipeline for
maxiumem performance.  Our tables are simple vectors, meaning they
have constant time read and write operations and use little memory
(giving the benefit of improved memory locality).  In addition, the
low--level read and write instructions require extremely few cycles to run.

We think this is a good implementation for fastidious hardware implementors.

\subsection{Example of a full networking flow}

Now we will look at how an example client request will flow through the
system.

First the client sends an IP--packet to a switch.
The switch will then fragment the packet, send the first and largest
fragment to its hosts and forward it to all the other switches\todo{Dette er
litt annerledes. Og vi må sørge for at når de to andre switchene får
pakken så sender de den ikke videre}.

The end--hosts will receive an IP--fragment, store it and wait for the
remaining fragment.

When the leader receives a client packet, it will initiate the Paxos
algorithm.  In our simplified version of Paxos, it will then send
\texttt{ACCEPT} messages to the other two switches.

These switches will then send out \texttt{LEARN}--messages.
When a switch has received a majority of \texttt{LEARN}s, it will proceed to
send the last fragment down to its hosts.  The hosts will then combine the
fragments and pass the packet to the application.

The applications will then process the packet and, optionally, send back a
reply.\todo{Skal vi se bort fra hvordan vi velger ut svar fra endesystemene
og sender svar til klienten? Eller skal vi bare legge inn flows sånn at
kun den som mottok opprinnelig pakke kan svare klienten?}

\todo{Legg inn illustrasjoner her}

What we have accomplished here is using Paxos for ordering the client
requests down to the hosts, so that each host will receive packets in the
same order.  To test this, we will run simulations where several clients
send requests to the hosts. After some time, the state of each host should
be equal to each other.

