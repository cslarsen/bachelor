\chapter{Implementation}
\label{chapter:implementation}

Based on the design in chapter \ref{chapter:design}, we will now look at
implementation details.

To be able to run Paxos in the switch, we must first extend the OpenFlow
Switch Specification with a new \textit{Paxos action}.
\index{flows!Paxos action}\index{OpenFlow!Paxos action}%
\index{Paxos!OpenFlow action}%
%
This will allow us to freely \textit{compose} flows that run the
Paxos algorithm as one part of their actions.
%
Finally, we must modify Open vSwitch so that we can run the new action.

\input{impl-flows.tex}

\todo{Flytt resten av kapittelet}

As discussed in \vref{chapter:theory.flow.table}, well-designed controllers
should install flows incrementally as they learn the network topology.
%
We must therefore first implement a system that works entirely without flow
entries. (Dette er en del av design-diskusjon, vi skal bare implementere det
her).

Next, we will implement flows in the system. As discussed previously, this
requires an extension to the OpenFlow-protocol and the switch software we
use, Open vSwitch. \todo{Dette må ha vært diskutert før}

Mer tekst: At vi har, i designet, extenda OpenFlow + Open vSwitch slik at vi
kan kjøre kode. Vi viser implementasjonen her, husk å skille på design og
implementasjon klart og tydelig.

\label{implementation.simplified.paxos}

We will implement algorithms \ref{algorithm:paxos.simple.acceptor} 
and \ref{algorithm:paxos.simple.learner} in a combination of OpenFlow
matches\index{OpenFlow!matching} and its extensions that were introduced
in \vref{chapter:extending.openflow}.

\input{learning-switch}

\section{Paxos Message Wire Format}

When exchanging Paxos messages between switches, we need a way to identify
them.
%
A well-known use of OpenFlow is to create entirely new, non-IP protocols
by matching on fields in the Ethernet header\index{Ethernet!header}
\cite[Example 4, p.~73]{McKeown:2008:OEI:1355734.1355746}.
%
We will tag Paxos messages with special values in the \textit{Ethernet
  type}-field\index{Ethernet!type}.
%
This field is two octets wide (i.e.,~16 bits), so we can use the most
significant one to mark packets as carrying Paxos messages, and the
least significant one for the kind of Paxos message (table
\ref{table:paxos.ethernet.type.encoding}).

\begin{table}[H]
  \centering
  \begin{tabular}{l|c|c|}
    \cline{2-3}
      & \multicolumn{2}{c|}{\textbf{Ethernet Type Field}} \\
      & \multicolumn{2}{c|}{16 bits} \\

    \hline
      \multicolumn{1}{|l|}{\textbf{Message Type}} &
      \textbf{Most Significant} &
      \textbf{Least Significant} \\

    \hline
      \multicolumn{1}{|l|}{\texttt{PAXOS JOIN}} &
      \texttt{0x7A} &
      \texttt{0x00} \\

    \hline
      \multicolumn{1}{|l|}{\texttt{PAXOS ACCEPT}} &
      \texttt{0x7A} &
      \texttt{0x01} \\

    \hline
      \multicolumn{1}{|l|}{\texttt{PAXOS LEARN}} &
      \texttt{0x7A} &
      \texttt{0x02} \\

    \hline
  \end{tabular}
  \caption{Encoding of \texttt{PAXOS}-messages in the \textit{Ethernet
    type} field.}
  \label{table:paxos.ethernet.type.encoding}
\end{table}

There is no particular reason for the specific values used in table
\ref{table:paxos.ethernet.type.encoding}, but since \texttt{ACCEPT}
and \texttt{LEARN} messages share the first parameters, they
could be bits that could both be turned on to send a combined
\texttt{ACCEPT-and-LEARN} message.  If both bits are zero, it becomes
a \texttt{JOIN} message.
%
We cannot use values below \texttt{0x600}, because that is used by
Ethernet to signify payload size.

Using the Ethernet type for identifying Paxos messages makes it very
convenient to match the different messages in OpenFlow's flow
tables\index{OpenFlow!flow table}.

We now have to define the payload structure for Paxos messages.
Table \ref{table:paxos.ethernet.packet} defines the parameters
each message type will contain.
%
It will consist of consecutive 32-bit values for storing parameters,
followed by the a full client packet in \texttt{ACCEPT}-messages.
%
Each type of message will trigger the corresponding algorithms in 
\vref{ch:simplifying.paxos}.  The \texttt{JOIN}-message is discussed in
chapter \ref{chapter:paxos.join.message}.

\begin{table}[H]
  \centering
  \begin{tabular}{l|l|c|c|c|}
    \hline
      \multirow{2}{*}{\dots} &
      \multicolumn{1}{c|}{\textbf{Ethernet Type}} &
      \multicolumn{2}{c|}{\textbf{Parameters}} &
      \textbf{Payload} \\

      &
      \multicolumn{1}{c|}{16 bits} &
      \multicolumn{1}{c}{32 bits} &
      \multicolumn{1}{c|}{32 bits} &
      \dots \\

    \hline
      \dots & \texttt{PAXOS JOIN}   & $node_{id}$ & MAC source &
        \multicolumn{1}{c}{} \\

    \hline
      \dots & \texttt{PAXOS CLIENT} & \textit{ignored} & \textit{ignored} &
          $v$ (client packet) \\

    \hline
      \dots & \texttt{PAXOS ACCEPT} & $n$ (round) & $seq$ (sequence) &
          $v$ (client packet) \\

    \hline
      \dots & \texttt{PAXOS LEARN}  & $n$ (round) & $seq$ (sequence) &
          \multicolumn{1}{c}{} \\

    \cline{1-4}
  \end{tabular}

  \caption{The structure of \acs{L2} Paxos messages.  Not shown her is
           the preceding Ethernet fields.}
  \label{table:paxos.ethernet.packet}
\end{table}
\index{Paxos!message structure}

At this point we should discuss what will happen when the round or sequence
number reaches the maximum number possible.
%
A good solution would be to program the Paxos nodes to allow values to
roll around to zero when passing the maximum value of $2^{31}-1$, so that
we would never run out of numbers.
%
This is a detail that is irrelevant for our stated goals, but a complete
implementation should naturally allow for infinite sequences.

\subsection{The \texttt{PAXOS ACCEPT} Message}
\label{chapter:paxos.accept.message}

The \texttt{ACCEPT} message contains the round and sequence numbers for the
embedded client packet.  They correspond to the variables $n$, $seq$ and
$v$ of the Paxos algorithms in chapter \vref{ch:simplifying.paxos},
respectively.

It will start algorithm \ref{algorithm:paxos.simple.acceptor} and send out
\texttt{LEARN}-messages, if the conditions are right.

Since it shares the first parameters with the \texttt{LEARN}-message, and
since only the leader send them out, a triggering ofshare the first parameters with the 
The \texttt{ACCEPT}-message share the first parameters with the
\texttt{LEARN}-message.
%


\subsection{The \texttt{PAXOS LEARN} Message}
\label{chapter:paxos.learn.message}

The \texttt{LEARN}-message triggers algorithm
\vref{algorithm:paxos.simple.learner}.

We have implemented this using multi-paxos, which will then update slots
with the number of learns.

%

\todo{Beskriv}

\subsection{The \texttt{PAXOS JOIN} Message}
\label{chapter:paxos.join.message}

When the system starts up, the switches need to announce themselves to each
other and learn which ports they are on.
%
To avoid having to rely on configuration files, we built a very simple
system for announcing the presence of Paxos nodes, loosely based on the
\acf{ARP}.

Each node will send out a \texttt{JOIN} containing its own node ID and
MAC-address,, sending it out on all ports with the Ethernet broadcast
destination of \texttt{ff:ff:ff:ff:ff:ff}.

When receiving a \texttt{JOIN}, the node will store the node ID and
MAC-address in a table and pass the MAC-address and source port number ot
the L2 learning switch as well.
%
If the MAC-address is not already in the table, it will reply to the sender
with a \texttt{JOIN}.

This will continue until a node knows about at least two other nodes---the
minimum required for Paxos execution.
%
If it does not know enough nodes after some seconds, it will send out a new
\texttt{JOIN} broadcast.
%
No other Paxos messages will be processed until enough nodes are known.

Since we are only interested in Paxos phase two, we do not perform any
leader election, but it would be natural to start Paxos leader election with
prepare and promise right after the \texttt{JOIN}-phase.
%
In our setup, we have simply designated a switch as leader, and we do not
support new nodes to join the Paxos network.

\section{The \texttt{PAXOS CLIENT} Message}
\label{chapter:paxos.client.message}

The \texttt{PAXOS CLIENT} message is used for distributing client packets
among the Paxos nodes.
%
To keep consistent with the established structure, the client packet itself
starts at an offset of 64 bits from the end of the Ethernet type field.
%
The two preceding parameters are unused.

Its intended use is to forward client packets to the Paxos leader, who will
then issue an \texttt{ACCEPT} message.
%
But this means that some Paxos nodes will see the same message several
times.  Referring to figure \vref{figure:paxos.on.switches}, if switch $S_3$
receives an incoming client packet, it will forward it in a \texttt{PAXOS
CLIENT} message to $S_2$, who will forward it to the leader $S_1$.
$S_1$ will then send back a \texttt{PAXOS ACCEPT} to $S_2$, whose L2 switch
will forward it to $S_3$ again.  All containing the same client packet.

Clearly, this design could be improved.
%
One possibility would be to generate a unique identifier for each incoming
client packet.  Each \texttt{PAXOS CLIENT}-message would carry it, and each
node would receive a copy of the message, storing it in a table with the
identifier as key.
%
The \texttt{PAXOS ACCEPT}-message would then contain this key instead of the
full client packet.
%
The identifier could be generated on each node by using the same
technique as for $crnd$ in equation \vref{equation:crnd_mod_N}.
%
Again we must stress that---while tempting---we have decided not to spend
time on building an optimal system.
%
Our goal is to build a distributed replication system using Paxos on the
switches, and along the way we uncover important result such as these that
could be investigated further.

\section{Handling Incoming Client Packets}
\label{chapter:incoming.client}

We need several OpenFlow matching rules\index{OpenFlow!matching} for all of this to work.

First, when a switch gets a client request (a packet from the
\acs{WAN}\index{wide-area network}) it needs
to add flow table entries that forwards it to all the other switches.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|}
    \hline
      \textbf{Switch} &
      \textbf{Flow Table Entry} \\

    \hline
      Leader & Store packet (or broadcast fragment to all hosts) \\
             & Send \texttt{ACCEPT} to slaves. \\

    \hline
      Slaves & Forward to leader \\

    \hline
  \end{tabular}

  \caption{OpenFlow flow table entries.}
  \label{table:paxos.flowtable.entries}
\end{table}
\todo{Hvis vi lagrer meldingen, eller uansett, så må vi jo vite current
  round number for at alt skal synke! Kan vi her sende noe til leder?}

Each switch need to store the full client packet---or parts of it, if we
use fragmentation to buffer the packets at the
end-hosts\index{end-host}---and then forward\index{forwarding}
it to the other switches.
\todo{Fragmentation trick, skal dette være med eller ikke?}

We also need entries for matching Paxos messages and react on these.
This is done by inserting entries that match on Ethernet type
\texttt{PAXOS} and ingress port from the leader.
The action will be to go to a new entry that looks at what kind of Paxos
message we have received.\footnote{An optimization trick would be to
combine the Paxos packet type identifier with the Paxos message type and put
them both in the Ethernet type-field\index{Ethernet!type-field}.  Then we could use existing OpenFlow
matching\index{OpenFlow!matching} instead of having to extract the Paxos message type.}

Finally, when matching on Paxos message types, we would execute 
special code using the new \texttt{run\_{}code}-action (see
    \vref{chapter:extending.openflow})
 and forward packets based on the return value from the code.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline \textbf{Action} & \textbf{Parameters} & \textbf{Description} \\
    \hline Fragment packet & buffer id, fragment offset & ... \\
    \hline Defragment packet & buffer id, buffer id & ... \\
    \hline Store fragment in table & buffer id & ... \\
    \hline Retrieve fragment from table & buffer id & ... \\
    \hline
  \end{tabular}

  \caption{New OpenFlow actions.}
  \label{table:openflow.new.actions}
\end{table}

We also need new OpenFlow protocol messages\index{OpenFlow!protocol
  messages} so that the controller is able
to install flows with these new actions\index{OpenFlow!extensions}.  However, because of the scope of
this thesis, we will simply store these actions directly in Open
vSwitch\index{Open vSwitch!flow table} and
pretend that these actions and flow entries came from the
controller.\footnote{While trivial, this takes a little work to do fully.
One would first have to modify the OpenFlow protocol with new
  actions\index{OpenFlow!extensions},
implement them and then do the same modifications in the controller.}

\section{The Switch Data Table}

Since each Paxos node needs to remember values for the round
number\index{round number}\index{Paxos!round number}, number
of nodes and so on, we propose that we add a simple table to each switch.

This is done by modifying the Open vSwitch source code\index{Open
vSwitch!source code}.

To conserve memory, we propose that each switch gets a table with 256
entries containing 32-bit values, for a total of 1024 bytes of memory.

\todo{Fiks, vi bruker litt annen tabell, må ha tabell per datapath (switch)
  og så per rundenummer osv}

\section{The Paxos Message Handlers}

\todo{Legg inn python kode-eksempler her}

\section{Example of a Full Networking Flow}

Now we will look at how an example client request will flow through the
system.

First the client sends an IP-packet to a switch.
The switch will then fragment the packet, send the first and largest
fragment to its hosts and forward it to all the other switches\todo{Dette er
litt annerledes. Og vi må sørge for at når de to andre switchene får
pakken så sender de den ikke videre}.

The end-hosts will receive an IP-fragment\index{fragmentation}, store it and
wait for the remaining fragment.

\begin{figure}
  \centering
  \begin{tikzpicture}[
      every node/.style={draw, circle},
      every on chain/.style={join},
      every join/.style={->}]

    {
      [start chain]
      \node [on chain] {$c_2$};
      \node [on chain=going below] {$S_2$};

      {
        [start branch=s1]
        \node [on chain=going left, node distance=4cm] {$S_1$};

        { [start branch]; \node [on chain=going below left]  {$h_1$}; }
        { [start branch]; \node [on chain=going below]       {$h_2$}; }
        { [start branch]; \node [on chain=going below right] {$h_3$}; }
      }

      {
        [start branch=s3]
        \node [on chain=going right, node distance=4cm] {$S_3$};

        { [start branch=h1]; \node [on chain=going below left]  {$h_7$}; }
        { [start branch=h2]; \node [on chain=going below]       {$h_8$}; }
        { [start branch=h3]; \node [on chain=going below right] {$h_9$}; }
      }

      { [start branch]; \node [on chain=going below left]  {$h_4$}; }
      { [start branch]; \node [on chain=going below]       {$h_5$}; }
      { [start branch]; \node [on chain=going below right] {$h_6$}; }

    }

  \end{tikzpicture}
  \caption{How a client message is forwarded to all end-hosts.}
  \label{figure:flow.client.forwarding}
\end{figure}
\todo{Få pilene til ikke å gå helt inn i noder (stealth?), og flytt
  side-switcher litt nedenfor switch i midten for å illustrere tid.}

When the leader receives a client packet, it will initiate the Paxos
algorithm.  In our simplified version of Paxos, it will then send
\texttt{ACCEPT} messages to the other two switches.

These switches will then send out \texttt{LEARN}-messages.
When a switch has received a majority of \texttt{LEARN}s, it will proceed to
send the last fragment down to its hosts.  The hosts will then combine the
fragments and pass the packet to the application.

The applications will then process the packet and, optionally, send back a
reply.\todo{Skal vi se bort fra hvordan vi velger ut svar fra endesystemene
og sender svar til klienten? Eller skal vi bare legge inn flows sånn at
kun den som mottok opprinnelig pakke kan svare klienten?}

\begin{figure}
  \centering
  \scriptsize
  \begin{tikzpicture}[>=stealth,x=1.2cm,y=1.2cm]
    \stdset{exec box color=white!20}
    \initstd
    \process{/S1}{$S_1$}
    \process{/S2}{$S_2$}
    \process{/S3}{$S_3$}
    \process{/c1}{$c_1$}
    \process{/hosts}{\textit{hosts}}

    % Groups
    \def\sw{/S1,/S3}
    \def\allsw{/S1,/S2,/S3}

    % Incoming client request
    \msg{/c1}{/S2}{Request}{v}{Forward}
    \mcast{/S2}{\allsw}{Forward}{v}{Store $v$}

    % ACCEPT
    \mcast{/S1}{\allsw}{Accept}{n,v}{On accept}

    % LEARN
    \alltoall{\allsw}{LEARN}{n,v}{On majority}

    % To hosts
    \mrcast{\allsw}{/hosts}{Request}{v}{Execute}

    \drawtimelines
  \end{tikzpicture}
  \caption{A client $c_1$ sends a request to the system. The message is
    forwarded to and stored on all switches.  The leader $S_1$ then sends out
      \texttt{ACCEPT} to all Paxos nodes.  They send \texttt{LEARN}s to
      all other switches.  When a switch has received \texttt{LEARN}s from a
      majority of nodes, it will send the message down to its
      \textit{hosts}, which then execute the client packet.  Not shown here
      is how we ensure that the client only gets back \textit{one} reply
      from the end-hosts.}
  \label{flow:simple}
\end{figure}
\todo{Husk å bruke EN IP-adresse utad mot klientene.}
\todo{Legg inn flere illustrasjoner her, oppdater graf og caption}
\todo{Merk, vi sender ut pakken til alle.. her kunne vi sendt til hosts
  direkte med fragmentering.. men vi kunne også bare sendt den ut til leader
    fra S2 og S3, og så sender leader ut hele pakken.. men vet ikke om det
    er nødvendig.. er like greit å gjøre med én gang? (det er sånn en kan
        finne ut hva som er best med benchmarks faktisk, som en alternativ
        konfigurasjon... merk også at vi KAN faktisk få synk-problemer her,
        så kanskje den BØR sendes til leder først for å få et offisielt
        pakkenummer????}

What we have accomplished here is using Paxos for
ordering\index{Paxos!ordering}\index{ordering} the client
requests down to the hosts, so that each host will receive packets in the
same order.  To test this, we will run simulations where several clients
send requests to the hosts. After some time, the state of each host should
be equal to each other.

\section{The Final Set of Flow Entries}
\label{chapter:final.flowtable}

\todo{oppdater dette}

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|}
    \hline \textbf{Match} & \textbf{Action} \\
    \hline From client & Fragment, store fragment 2 w/crnd, send fragment 1 to hosts \\
                       & Execute send-accept program \\
    \hline From host & Forward to client (TODO: Ignore, only allow one reply) \\
    \hline PAXOS JOIN  & Store MAC address and node id of switch \\
    \hline PAXOS LEARN & Execute program on-learn \\
    \hline
  \end{tabular}
  \caption{The final flow table for the Paxos leader.}
  \label{table:complete.match.leader}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|}
    \hline \textbf{Match} & \textbf{Action} \\
    \hline From client & Fragment, store fragment 2 w/crnd, send fragment 1 to hosts \\
                       & Forward to leader \\
    \hline From host & Forward to client (TODO: Ignore, only allow one reply) \\
    \hline PAXOS JOIN from any & Store MAC address, node id and leader-flag \\
    \hline PAXOS LEARN from any & Execute program on-learn \\
    \hline PAXOS ACCEPT from leader & Execute program on-accept \\
    \hline
  \end{tabular}
  \caption{The final flow table for Paxos slaves.}
  \label{table:complete.match.slave}
\end{table}

\todo{Legg inn diagrammer for nettverksflyt, dette gjelder andre steder
  også.}
