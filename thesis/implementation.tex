\chapter{Implementation of simplified Paxos}
\label{implementation.simplified.paxos}

We will implement algorithms \ref{algorithm:paxos.simple.acceptor} 
and \ref{algorithm:paxos.simple.learner} in a combination of OpenFlow
matches and Forth.

\input{learning-switch}

\section{The structure of Paxos--messages}

For transmitting Paxos--messages between the switches, we don't need to use
the IP--protocol.

A nice trick in OpenFlow is just to use Ethernet packets
and identify them by marking the \textit{Ethernet type} field with a special value.
Since the Ethernet type field is two octets wide (i.e.,~16 bits), we can
mark messages as being \texttt{PAXOS} in the most significant octet, and the
type of Paxos message in its least significant octet:

\begin{table}[H]
  \centering
  \begin{tabular}{l|c|c|}
    \cline{2-3}
      & \multicolumn{2}{c|}{\textbf{Ethernet type field}} \\
      & \multicolumn{2}{c|}{16 bits} \\

    \hline
      \multicolumn{1}{|l|}{\textbf{Message type}} &
      \textbf{Most significant} &
      \textbf{Least significant} \\

    \hline
      \multicolumn{1}{|l|}{\texttt{PAXOS HELLO}} &
      \texttt{0x7A} &
      \texttt{0x05} \\

    \hline
      \multicolumn{1}{|l|}{\texttt{PAXOS ACCEPT}} &
      \texttt{0x7A} &
      \texttt{0x06} \\

    \hline
      \multicolumn{1}{|l|}{\texttt{PAXOS LEARN}} &
      \texttt{0x7A} &
      \texttt{0x07} \\

    \hline
  \end{tabular}
  \caption{Encoding of \texttt{PAXOS}--messages in the \textit{Ethernet
    type} field.}
  \label{table:paxos.ethernet.type.encoding}
\end{table}

There is no particular reason for the specific values used in table
\ref{table:paxos.ethernet.type.encoding}\footnote{Although one can argue
that \texttt{0x\underline{7A}} \texttt{0\underline{x05}} spells out \texttt{PAXOS}.}.

The reason for marking Paxos messages in the Ethernet header is that it
makes it very easy to match such packets in the flow tables.  It also
enables us do use whatever we want as payload\footnote{The payload could
even be the complete client packet as--is, with full IP and TCP/UDP
headers.  We haven't done that in this thesis, though.}.  On the downside,
we can only send such messages on Ethernet networks.

The packet payload will then consist of consecutive 32--bit values.
This will make it easy for the switches to extract the contents.
Of course, since we don't use IP, we can only exchange these packets on a
local network.  If we wanted to distribute the switches across the network,
we would have to use IP.

\begin{table}[H]
  \centering
  \begin{tabular}{l|l|l|l|l|l|}
    \hline
      \multirow{2}{*}{\dots} &
      \multicolumn{1}{|c|}{\textbf{Ethernet type}} &
      \multirow{2}{*}{\dots} &
      \multicolumn{2}{|c|}{\textbf{Payload}} \\

      &
      \multicolumn{1}{|c|}{16 bits} &
      &
      \multicolumn{2}{|c|}{64 bits} \\

    \hline
      \dots & \texttt{PAXOS HELLO}  &
      \dots & $node_{id}$ & $isleader$ \\

    \hline
      \dots & \texttt{PAXOS ACCEPT} &
      \dots & $rnd$ & $packet_{id}$ \\

    \hline
      \dots & \texttt{PAXOS LEARN} &
      \dots & $rnd$ & $packet_{id}$ \\

    \hline
  \end{tabular}

  \caption{The structure of Paxos messages in Ethernet packets.}
  \label{table:paxos.ethernet.packet}
\end{table}

The payload is simply a flat vector of unsigned 32--bit integers.  The length
of the vectors are determined by the type of message.  For practical
purposes (being that we are developing on Intel x86--CPUs) we'll use
little--endian numbers, although a better alternative would be to go for
\textit{network order}---which is big--endian\footnote{Our goal is to
demonstrate the \textit{feasibility} of our approach, so we have deemed it
wise to avoid having to deal with endianness--conversion.}.

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline \textbf{Type} & \textbf{Parameter 1} & \textbf{Parameter 2} \\
           16 bits & 32 bits & 32 bits \\
    \hline \texttt{PAXOS HELLO} & $node_{id}$ & $isleader$ \\
    \hline \texttt{PAXOS ACCEPT} & $rnd$ & $packet_{id}$ \\
    \hline \texttt{PAXOS LEARN} & $rnd$ & $packet_{id}$ \\
    \hline
  \end{tabular}
  \caption{Structure of Paxos message payloads}
  \label{table:paxos.payload.structure}
\end{table}

At this point we
could debate whether to support wider values, e.g.~64--bit values.
This is not important for our demonstration.  A good solution would be to
allow for these values to roll around to zero again, but that would require
some changes to our algorithms.

The corresponding structure in C, after extracting the Ethernet type and
parameters, would be

\begin{lstlisting}[
  caption={C structure for a Paxos message with parameters.},
  label={struct:paxos.message}]
struct paxos_message {
    uint16_t paxos_type;
    uint32_t params[2];
};
\end{lstlisting}

As the structure above is defined in the C language, the padding will be
implicit by the rules of the C programming language standard and
platform\footnote{On my OS X x86\_{}64 computer, the size of the structure
is 12 bytes---i.e.,~no padding, as it aligns on a word boundary.}.

The reason we use two octets (16 bits) for the \texttt{paxos\_{}type} field
is that this will be actually be placed in the \textit{Ethernet type} field
of an Ethernet packet, and this is 16 bits wide \cite{IEEE.802.3}.

If we intended to implement full Paxos, we could simply add more message
types to the above structure and more parameters.

\subsection{The \texttt{PAXOS HELLO} message}

When the system starts up, the switches need to announce themselves to each
other and learn which ports they are on.  For this we use the
\texttt{HELLO}--message given in table \ref{table:paxos.ethernet.packet}.
In a full Paxos implementation, the system would then perform leader
election.  That is out of scope for this thesis, so we will just designate
the first switch as the leader.  We also simplify the system further by
letting each switch know ahead how many other Paxos nodes there are---and
this will be static throughout the lifetime of the system\footnote{A full
Paxos implementation would also implement functionality for keeping tabs
on the liveness of each node.  OpenFlow has some limited support of
notifying the controllers when link status changes.  Also, a
production--quality system would allow for nodes to join and leave the
system.}.

\section{Handling incoming client messages}
\label{chapter:incoming.client}

We need several OpenFlow matching rules for all of this to work.

First, when a switch gets a client request (a packet from the WAN) it needs
to add flow table entries that forwards it to all the other switches.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|}
    \hline
      \textbf{Switch} &
      \textbf{Flow Table Entry} \\

    \hline
      Leader & Store packet (or broadcast fragment to all hosts) \\
             & Send \texttt{ACCEPT} to slaves. \\

    \hline
      Slaves & Forward to leader \\

    \hline
  \end{tabular}

  \caption{OpenFlow flow table entries.}
  \label{table:paxos.flowtable.entries}
\end{table}
\todo{Hvis vi lagrer meldingen, eller uansett, så må vi jo vite current
  round number for at alt skal synke! Kan vi her sende noe til leder?}

Each switch need to store the full client packet---or parts of it, if we
use fragmentation to buffer the packets at the end--hosts---and then forward
it to the other switches.
\todo{Fragmentation trick, skal dette være med eller ikke?}

We also need entries for matching Paxos messages and react on these.
This is done by inserting entries that match on Ethernet type
\texttt{PAXOS} and ingress port from the leader.
The action will be to go to a new entry that looks at what kind of Paxos
message we have received\footnote{An optimization trick would be to
combine the Paxos packet type identifier with the Paxos message type and put
them both in the Ethernet type field.  Then we could use existing OpenFlow
matching instead of having to extract the Paxos message type.}.

Finally, when matching on Paxos message types, we would execute the Forth
bytecode and forward packets based on the return value from the code.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|}
    \hline \textbf{Action} & \textbf{Parameters} & \textbf{Description} \\
    \hline Fragment packet & buffer id, fragment offset & ... \\
    \hline Defragment packet & buffer id, buffer id & ... \\
    \hline Store fragment in table & buffer id & ... \\
    \hline Retrieve fragment from table & buffer id & ... \\
    \hline
  \end{tabular}

  \caption{New OpenFlow actions.}
  \label{table:openflow.new.actions}
\end{table}

We also need new OpenFlow protocol messages so that the controller is able
to install flows with these new actions.  However, because of the scope of
this thesis, we will simply store these actions directly in OpenVSwitch and
pretend that these actions and flow entries came from the
controller\footnote{While trivial, this takes a little work to do fully.
One would first have to modify the OpenFlow protocol with new actions,
implement them and then do the same modifications on the controller.}.

\section{The switch data table}

Since each Paxos node needs to remember values for the round number, number
of nodes and so on, we propose that we add a simple table to each switch.

This is done by modifying the OpenVSwitch source code.

To conserve memory, we propose that each switch gets a table with 256
entries containing 32--bit values, for a total of 1024 bytes of memory.

OpenVSwitch needs to expose internally functions for manipulating this table
to Forth.  Our Forth code can then store the node id at location 0, round
numbers at location 1 and so on.

\section{The Paxos message handlers}

The \textbf{pickNext} algorithm is trivial and can be translated directly to
Forth.  The value $|N|$ needs to be set in an initialization word,
while $crnd$ must be synchronized with its value in the switch's data table.

\begin{lstlisting}[
  caption={Implementation of \textbf{pickNext} (algorithm
      \ref{algorithm:paxos.simple.pickNext}) in Forth.},
  label={program.forth.pickNext}]
variable |N|
variable crnd

: pickNext ( -- crnd + |N| )
    \ Calculate the next value of crnd
    crnd @ |N| @ + ;
\end{lstlisting}

The handling of \texttt{ACCEPT}--messages will be done in a combination of
Forth code and OpenFlow matching.  When the controller learns which port the
leader is on, it will install a flow entry to match on
\texttt{ACCEPT}--messages from the Paxos leader:

\begin{table}[H]
  \centering
  \begin{tabular}{l|l|l|l|l|}
    \hline
      \dots &
      \textbf{Ethernet type} &
      \dots &
      \textbf{Ethernet source} &
      \textbf{Action}
      \\
    \hline
      \dots &
      $\texttt{PAXOS ACCEPT}$
      & \dots
      & Leader MAC address
      & Execute program \ref{program:forth.on-accept}
      \\
    \hline
  \end{tabular}
  \caption{Flow entry matching rule for \texttt{ACCEPT}--messages.}
  \label{table:matching.simple.accept}
\end{table}

The code itself is straight--forward, except for the fact that $rnd$ and
$vval$ are not local Forth variables, but are actually stored in the
switch's data table\footnote{There are several ways of achieving this.
Either we can create Forth words for accessing the data table directly.
This method is explicit and easy.  The other way is to add code to have
the Forth VM mark certain variables to be automatically accessed from the
data table.}.

\begin{lstlisting}[
  caption={Implementation of \texttt{on\_{}accept}},
  label={program:forth.on-accept}]
variable node_id
variable vval

\ MAC addresses for switches
\ These are stored in the data table at known locations
10 data.table@ mac.s1
11 data.table@ mac.s2
12 data.table@ mac.s3

\ Duplicate topmost two items
: dup2 ( a b -- a b a b )
    over over ;

: on_accept ( n v -- )
    swap dup                             ( n v -- v n n )
    rnd @ >= if                          ( v n n -- v n )
        dup rnd !                        ( v n -- v n ; save rnd)
        over vval !                      ( v n -- v n ; save vval)
        swap                             ( v n -- n v )

        \ Send LEARN to switches
        dup2 mac.s1 paxos.learn openflow.flood
        dup2 mac.s2 paxos.learn openflow.flood
        dup2 mac.s3 paxos.learn openflow.flood
    else
        drop drop ( v n -- )
    then ;
\end{lstlisting}

Program \ref{program:forth.on-accept} \vpageref{program:forth.on-accept}
uses the word \texttt{paxos.learn} (program \ref{program:forth.paxos.learn})
to construct an Ethernet packet with the
Ethernet type set to \texttt{PAXOS.LEARN} and its payload containing the two
32--bit values $\langle v, n \rangle$.

\begin{lstlisting}[
  caption={Implementation of \texttt{paxos.learn} for creating a
    \texttt{PAXOS LEARN} packet},
  label={program:forth.paxos.learn}]
: paxos.learn ( addr n v -- Ethernet packet )
    2 paxos.pack32          ( addr n v -- addr payload )
    paxos.eth_type.learn    ( addr payload -- addr payload ethtype )
    swap paxos.eth_packet ; ( addr payload ethtype -- ethernet_packet )
\end{lstlisting}

The payload is created using
\texttt{2 paxos.pack32} (\textit{"pack two 32--bit values"}) and the message
is sent out using \texttt{openflow.flood}.  The two previous words are calls
into C.

In the same manner, we can create definitions for \texttt{paxos.accept} and
\texttt{paxos.hello} (programs \ref{program:forth.paxos.accept} and
\ref{program:forth.paxos.hello}, respectively).

\begin{lstlisting}[
  caption={Implementation of \texttt{paxos.accept} for creating a
    \texttt{PAXOS ACCEPT} packet},
  label={program:forth.paxos.accept}]
: paxos.accept ( addr n v -- Ethernet packet )
    2 paxos.pack32
    paxos.eth_type.accept
    swap paxos.eth_packet ;
\end{lstlisting}

\begin{lstlisting}[
  caption={Implementation of \texttt{paxos.hello} for creating a
    \texttt{PAXOS HELLO} packet},
  label={program:forth.paxos.hello}]
: paxos.hello ( addr node isleader -- Ethernet packet )
    2 paxos.pack32
    paxos.eth_type.hello
    swap paxos.eth_packet ;
\end{lstlisting}

Continuing in this manner, we can make code for \texttt{on\_{}learn}.

\begin{lstlisting}[
  caption={Implementation of \texttt{on\_{}learn} in Forth (algorithm
      \ref{algorithm:paxos.simple.learner}).},
  label={program:forth.on-learn}
]
\ Start of number of received learns
\ E.g. at table[100] we have number of learns for rnd=0,
\ etc.
100 constant index.learned

\ Calculate location of rnd data table
: rnd.index ( rnd -- index )
  index.learned + ;

\ Increments number of learns for given round number
: inc.learns   ( rnd -- )
    rnd.index  ( rnd -- index )
    dup table@ ( index -- index value )
    +1 swap    ( index value -- value+1 index )
    table!     ( value+1 index -- ) ;

: got.majority?       ( rnd -- true/false )
    rnd.index table@  ( rnd -- learns )
    |N| 2 / > if      \ Is learns > |N|/2 ?
        -1            \ True
    else
         0            \ False
    then ;

: on_learn ( n v -- )
    over got.majority? if
        get.fragment openflow.flood.hosts
    else
        drop
    then ;
\end{lstlisting}

\section{Example of a full networking flow}

Now we will look at how an example client request will flow through the
system.

First the client sends an IP--packet to a switch.
The switch will then fragment the packet, send the first and largest
fragment to its hosts and forward it to all the other switches\todo{Dette er
litt annerledes. Og vi må sørge for at når de to andre switchene får
pakken så sender de den ikke videre}.

The end--hosts will receive an IP--fragment, store it and wait for the
remaining fragment.

\begin{figure}
  \centering
  \begin{tikzpicture}[
      every node/.style={draw, circle},
      every on chain/.style={join},
      every join/.style={->}]

    {
      [start chain]
      \node [on chain] {$c_2$};
      \node [on chain=going below] {$S_2$};

      {
        [start branch=s1]
        \node [on chain=going left, node distance=4cm] {$S_1$};

        { [start branch]; \node [on chain=going below left]  {$h_1$}; }
        { [start branch]; \node [on chain=going below]       {$h_2$}; }
        { [start branch]; \node [on chain=going below right] {$h_3$}; }
      }

      {
        [start branch=s3]
        \node [on chain=going right, node distance=4cm] {$S_3$};

        { [start branch=h1]; \node [on chain=going below left]  {$h_7$}; }
        { [start branch=h2]; \node [on chain=going below]       {$h_8$}; }
        { [start branch=h3]; \node [on chain=going below right] {$h_9$}; }
      }

      { [start branch]; \node [on chain=going below left]  {$h_4$}; }
      { [start branch]; \node [on chain=going below]       {$h_5$}; }
      { [start branch]; \node [on chain=going below right] {$h_6$}; }

    }

  \end{tikzpicture}
  \caption{How a client message is forwarded to all end--hosts.}
  \label{figure:flow.client.forwarding}
\end{figure}
\todo{Få pilene til ikke å gå helt inn i noder (stealth?), og flytt
  side-switcher litt nedenfor switch i midten for å illustrere tid.}

When the leader receives a client packet, it will initiate the Paxos
algorithm.  In our simplified version of Paxos, it will then send
\texttt{ACCEPT} messages to the other two switches.

These switches will then send out \texttt{LEARN}--messages.
When a switch has received a majority of \texttt{LEARN}s, it will proceed to
send the last fragment down to its hosts.  The hosts will then combine the
fragments and pass the packet to the application.

The applications will then process the packet and, optionally, send back a
reply.\todo{Skal vi se bort fra hvordan vi velger ut svar fra endesystemene
og sender svar til klienten? Eller skal vi bare legge inn flows sånn at
kun den som mottok opprinnelig pakke kan svare klienten?}

\begin{figure}
  \centering
  \scriptsize
  \begin{tikzpicture}[>=stealth]
    \stdset{exec box color=white!20}
    \initstd
    \process{/S1}{$S_1$}
    \process{/S2}{$S_2$}
    \process{/S3}{$S_3$}
    \process{/c1}{$c_1$}
    \process{/hosts}{\textit{hosts}}

    % Groups
    \def\sw{/S1,/S3}
    \def\allsw{/S1,/S2,/S3}

    % Incoming client request
    \msg{/c1}{/S2}{Request}{v}{Request}
    \mcast{/S2}{\allsw}{Forward}{v}{Store $v$}

    % ACCEPT
    \mcast{/S1}{\sw}{Accept}{n,v}{On accept}

    % LEARN
    \alltoall{\allsw}{LEARN}{n,v}{On majority}

    % To hosts
    \mrcast{\allsw}{/hosts}{Request}{v}{Execute}

    \drawtimelines
  \end{tikzpicture}
  \caption{A client $c_1$ sends a request to the system. The message is
    forwarded to and stored on all switches.  The leader $S_1$ then sends out
      \texttt{ACCEPT} to all Paxos nodes.  They reply with \texttt{LEARN} to
      all other switches.  When a switch has received \texttt{LEARN}s from a
      majority of nodes, it will send the message down to its
      \textit{hosts}, which then execute the client package.  Not shown here
      is how we ensure that the client only gets back \textit{one} reply
      from the end--hosts.}
  \label{flow:simple}
\end{figure}
\todo{Legg inn flere illustrasjoner her, oppdater graf og caption}

What we have accomplished here is using Paxos for ordering the client
requests down to the hosts, so that each host will receive packets in the
same order.  To test this, we will run simulations where several clients
send requests to the hosts. After some time, the state of each host should
be equal to each other.

\section{The final set of flow entries}
\label{chapter:final.flowtable}

% Reglene må bli sånn
% - slaver: fra klient? fragment, send frag1 til hosts, forward til alle %   switcher
% - fra host? ...
% - paxos hello? store mac address (l2 learning), address of leader i table


\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|}
    \hline \textbf{Match} & \textbf{Action} \\
    \hline From client & Fragment, store fragment 2 w/crnd, send fragment 1 to hosts \\
                       & Execute send--accept program \\
    \hline From host & Forward to client (TODO: Ignore, only allow one reply) \\
    \hline PAXOS HELLO & Store MAC address and node id of switch \\
    \hline PAXOS LEARN & Execute program on--learn \\
    \hline
  \end{tabular}
  \caption{The final flow table for the Paxos leader.}
  \label{table:complete.match.leader}
\end{table}

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|}
    \hline \textbf{Match} & \textbf{Action} \\
    \hline From client & Fragment, store fragment 2 w/crnd, send fragment 1 to hosts \\
                       & Forward to leader \\
    \hline From host & Forward to client (TODO: Ignore, only allow one reply) \\
    \hline PAXOS HELLO from any & Store MAC address, node id and leader--flag \\
    \hline PAXOS LEARN from any & Execute program on--learn \\
    \hline PAXOS ACCEPT from leader & Execute program on--accept \\
    \hline
  \end{tabular}
  \caption{The final flow table for Paxos slaves.}
  \label{table:complete.match.slave}
\end{table}

\todo{Legg inn diagrammer for nettverksflyt, dette gjelder andre steder
  også.}
