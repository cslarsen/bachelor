\chapter{Implementation}
\label{chapter:implementation}

Based on the design in chapter \ref{chapter:design}, we will now look at
implementation details.

To be able to run Paxos in the switch, we must first extend the OpenFlow
Switch Specification with a new \textit{Paxos action}.
\index{flows!Paxos action}\index{OpenFlow!Paxos action}%
\index{Paxos!OpenFlow action}%
%
This will allow us to freely \textit{compose} flows that run the
Paxos algorithm as one part of their actions.
%
Finally, we must modify Open vSwitch so that we can run the new action.

\input{impl-flows.tex}

As discussed in \vref{chapter:theory.flow.table}, well-designed controllers
should install flows incrementally as they learn the network topology.
%
We must therefore first implement a system that works entirely without flow
entries. (Dette er en del av design-diskusjon, vi skal bare implementere det
her).

Next, we will implement flows in the system. As discussed previously, this
requires an extension to the OpenFlow-protocol and the switch software we
use, Open vSwitch.

Mer tekst: At vi har, i designet, extenda OpenFlow + Open vSwitch slik at vi
kan kjøre kode. Vi viser implementasjonen her, husk å skille på design og
implementasjon klart og tydelig.

\label{implementation.simplified.paxos}

We will implement algorithms \ref{algorithm:paxos.simple.acceptor} 
and \ref{algorithm:paxos.simple.learner} in a combination of OpenFlow
matches\index{OpenFlow!matching} and its extensions that were introduced
in \vref{chapter:extending.openflow}.

\input{learning-switch}

\section{Paxos Message Wire Format}

When exchanging Paxos messages between switches, we need a way to identify
them.
%
A well-known use of OpenFlow is to create entirely new, non-IP protocols
by matching on fields in the Ethernet header\index{Ethernet!header}
\cite[Example 4, p.~73]{McKeown:2008:OEI:1355734.1355746}.
%
We will tag Paxos messages with special values in the \textit{Ethernet
  type}-field\index{Ethernet!type}.
%
This field is two octets wide (i.e.,~16 bits), so we can use the most
significant one to mark packets as carrying Paxos messages, and the
least significant one for the kind of Paxos message (table
\ref{table:paxos.ethernet.type.encoding}).

\begin{table}[H]
  \centering
  \begin{tabular}{l|c|c|}
    \cline{2-3}
      & \multicolumn{2}{c|}{\textbf{Ethernet Type Field}} \\
      & \multicolumn{2}{c|}{16 bits} \\

    \hline
      \multicolumn{1}{|l|}{\textbf{Message Type}} &
      \textbf{Most Significant} &
      \textbf{Least Significant} \\

    \hline
      \multicolumn{1}{|l|}{\texttt{PAXOS JOIN}} &
      \texttt{0x7A} &
      \texttt{0x00} \\

    \hline
      \multicolumn{1}{|l|}{\texttt{PAXOS ACCEPT}} &
      \texttt{0x7A} &
      \texttt{0x01} \\

    \hline
      \multicolumn{1}{|l|}{\texttt{PAXOS LEARN}} &
      \texttt{0x7A} &
      \texttt{0x02} \\

    \hline
  \end{tabular}
  \caption{Encoding of \texttt{PAXOS} messages in the \textit{Ethernet
    type} field.}
  \label{table:paxos.ethernet.type.encoding}
\end{table}

There is no particular reason for the specific values used in table
\ref{table:paxos.ethernet.type.encoding}, but since \texttt{ACCEPT}
and \texttt{LEARN} messages share the first parameters, they
could be bits that could both be turned on to send a combined
\texttt{ACCEPT-and-LEARN} message.  If both bits are zero, it becomes
a \texttt{JOIN} message.
%
We cannot use values below \texttt{0x600}, because that is used by
Ethernet to signify payload size.

Using the Ethernet type for identifying Paxos messages makes it very
convenient to match the different messages in OpenFlow's flow
tables\index{OpenFlow!flow table}.

We now have to define the payload structure for Paxos messages.
Table \ref{table:paxos.ethernet.packet} defines the parameters
each message type will contain.
%
It will consist of consecutive 32-bit values for storing parameters,
followed by the a full client packet in \texttt{ACCEPT} messages.
%
Each type of message will trigger the corresponding algorithms in 
\vref{ch:simplifying.paxos}.  The \texttt{JOIN} message is discussed in
chapter \ref{chapter:paxos.join.message}.

\begin{table}[H]
  \centering
  \begin{tabular}{l|l|c|c|c|}
    \hline
      \multirow{2}{*}{\dots} &
      \multicolumn{1}{c|}{\textbf{Ethernet Type}} &
      \multicolumn{2}{c|}{\textbf{Parameters}} &
      \textbf{Payload} \\

      &
      \multicolumn{1}{c|}{16 bits} &
      \multicolumn{1}{c}{32 bits} &
      \multicolumn{1}{c|}{32 bits} &
      \dots \\

    \hline
      \dots & \texttt{PAXOS JOIN}   & $node_{id}$ & MAC source &
        \multicolumn{1}{c}{} \\

    \hline
      \dots & \texttt{PAXOS CLIENT} & \textit{ignored} & \textit{ignored} &
          $v$ (client packet) \\

    \hline
      \dots & \texttt{PAXOS ACCEPT} & $n$ (round) & $seq$ (sequence) &
          $v$ (client packet) \\

    \hline
      \dots & \texttt{PAXOS LEARN}  & $n$ (round) & $seq$ (sequence) &
          \multicolumn{1}{c}{} \\

    \cline{1-4}
  \end{tabular}

  \caption{The structure of \acs{L2} Paxos messages.  Not shown her is
           the preceding Ethernet fields.}
  \label{table:paxos.ethernet.packet}
\end{table}
\index{Paxos!message structure}

At this point we should discuss what will happen when the round or sequence
number reaches the maximum number possible.
%
A good solution would be to program the Paxos nodes to allow values to
roll around to zero when passing the maximum value of $2^{31}-1$, so that
we would never run out of numbers.
%
This is a detail that is irrelevant for our stated goals, but a complete
implementation should naturally allow for infinite sequences.

\subsection{The \texttt{PAXOS ACCEPT} Message}
\label{chapter:paxos.accept.message}

The \texttt{ACCEPT} message contains the round and sequence numbers for the
embedded client packet.  They correspond to the variables $n$, $seq$ and
$v$ of the Paxos algorithms in chapter \vref{ch:simplifying.paxos},
respectively.

It will start algorithm \ref{algorithm:paxos.simple.acceptor} and send out
\texttt{LEARN} messages, if the conditions are right.

Since it shares the first parameters with the \texttt{LEARN} message, and
since only the leader send them out, a triggering ofshare the first parameters with the 
The \texttt{ACCEPT} message share the first parameters with the
\texttt{LEARN} message.
%


\subsection{The \texttt{PAXOS LEARN} Message}
\label{chapter:paxos.learn.message}

The \texttt{LEARN} message triggers algorithm
\vref{algorithm:paxos.simple.learner}.

We have implemented this using multi-paxos, which will then update slots
with the number of learns.

\subsection{The \texttt{PAXOS JOIN} Message}
\label{chapter:paxos.join.message}

When the system starts up, the switches need to announce themselves to each
other and learn which ports they are on.
%
To avoid having to rely on configuration files, we built a very simple
system for announcing the presence of Paxos nodes, loosely based on the
\acf{ARP}.

Each node will send out a \texttt{JOIN} containing its own node ID and
MAC-address,, sending it out on all ports with the Ethernet broadcast
destination of \texttt{ff:ff:ff:ff:ff:ff}.

When receiving a \texttt{JOIN}, the node will store the node ID and
MAC-address in a table and pass the MAC-address and source port number ot
the L2 learning switch as well.
%
If the MAC-address is not already in the table, it will reply to the sender
with a \texttt{JOIN}.

This will continue until a node knows about at least two other nodes---the
minimum required for Paxos execution.
%
If it does not know enough nodes after some seconds, it will send out a new
\texttt{JOIN} broadcast.
%
No other Paxos messages will be processed until enough nodes are known.

Since we are only interested in Paxos phase two, we do not perform any
leader election, but it would be natural to start Paxos leader election with
prepare and promise right after the \texttt{JOIN}-phase.
%
In our setup, we have simply designated a switch as leader, and we do not
support new nodes to join the Paxos network.

\section{The \texttt{PAXOS CLIENT} Message}
\label{chapter:paxos.client.message}

The \texttt{PAXOS CLIENT} message is used for distributing client packets
among the Paxos nodes.
%
To keep consistent with the established structure, the client packet itself
starts at an offset of 64 bits from the end of the Ethernet type field.
%
The two preceding parameters are unused.

Its intended use is to forward client packets to the Paxos leader, who will
then issue an \texttt{ACCEPT} message.
%
But this means that some Paxos nodes will see the same message several
times.  Referring to figure \vref{figure:paxos.on.switches}, if switch $S_3$
receives an incoming client packet, it will forward it in a \texttt{PAXOS
CLIENT} message to $S_2$, who will forward it to the leader $S_1$.
$S_1$ will then send back a \texttt{PAXOS ACCEPT} to $S_2$, whose L2 switch
will forward it to $S_3$ again.  All containing the same client packet.

Clearly, this design could be improved.
%
One possibility would be to generate a unique identifier for each incoming
client packet.  Each \texttt{PAXOS CLIENT} message would carry it, and each
node would receive a copy of the message, storing it in a table with the
identifier as key.
%
The \texttt{PAXOS ACCEPT} message would then contain this key instead of the
full client packet.
%
The identifier could be generated on each node by using the same
technique as for $crnd$ in equation \vref{equation:crnd_mod_N}.
%
Again we must stress that---while tempting---we have decided not to spend
time on building an optimal system.
%
Our goal is to build a distributed replication system using Paxos on the
switches, and along the way we uncover important result such as these that
could be investigated further.

\section{Handling Incoming Client Packets}
\label{chapter:incoming.client}

First, when a switch gets a client packet it needs to add flow table
entries that forward it to all the other switches.
We need several OpenFlow matching rules\index{OpenFlow!matching} for all of this to work.
%
Note that all Paxos nodes except the leader will be called for
\textit{Paxos slaves} from now on.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|}
    \hline
      \textbf{Switch} &
      \textbf{Flow Table Entry} \\

    \hline
      Leader & Store packet \\
             & Send \texttt{ACCEPT} to slaves. \\

    \hline
      Slaves & Forward to leader \\

    \hline
  \end{tabular}

  \caption{OpenFlow flow table entries.}
  \label{table:paxos.flowtable.entries}
\end{table}

Each switch need to store the full client packet and then
forward\index{forwarding} it to the other switches.

We also need entries for matching Paxos messages and their respective
actions.
%
This is done by inserting entries that match on Ethernet type
\texttt{PAXOS} and ingress port from the leader.
The action will be to go to a new entry that looks at what kind of Paxos
message we have received.

Finally, when matching on Paxos message types, we would execute 
special code using the new \texttt{run\_{}code}-action (see
    \vref{chapter:extending.openflow})
 and forward packets based on the return value from the code.

We also need new OpenFlow protocol messages\index{OpenFlow!protocol
messages} so that the controller is able to install flows with these new
actions\index{OpenFlow!extensions}.
%
However, to save time, we will simply install these flows by using the
\texttt{ovs-ofctl} command-line program from the Open vSwitch-distribution.
%
While Open vSwitch has been modified to support the new OpenFlow actions in
the switch-to-controller protocol, we would have to modify the POX framework
to be able to parse such messages, update its feature table and so on.
%
This is considered trivial to do, but time consuming and irrelevant to our
task.

\input{paxos-controller.tex}

\section{Example of a Full Networking Flow}

In figure \vref{figure:flow.client.forwarding}, we show the complete flow of
a client packet through the system, running through the Paxos algorithm on
the switches and finally being processed and forwarded to the end-hosts.
Not shown here is any reply from these end-hosts.

\begin{figure}
  \centering
  \scriptsize
  \begin{tikzpicture}[>=stealth,x=1.2cm,y=1.2cm]
    \stdset{exec box color=white!20}
    \initstd
    \process{/S1}{$S_1$}
    \process{/S2}{$S_2$}
    \process{/S3}{$S_3$}
    \process{/c1}{$c_1$}
    \process{/hosts}{\textit{hosts}}

    % Groups
    \def\sw{/S1,/S3}
    \def\allsw{/S1,/S2,/S3}

    % Incoming client request
    \msg{/c1}{/S1}{Packet}{v}{On client}

    % ACCEPT
    \mcast{/S1}{\allsw}{Accept}{n,seq,v}{On accept, store $v$}

    % LEARN
    \alltoall{\allsw}{LEARN}{n,seq}{On learn, majority}

    % To hosts
    \mrcast{\allsw}{/hosts}{Packet}{v}{Process $v$}

    \drawtimelines
  \end{tikzpicture}
  \caption{A client $c_1$ sends a request to the system. The message is
    forwarded to the Paxos leader $S_1$, which sends the client packet as a
      parameter to an accept message, incrementing the current sequence
      number $seq$.  All nodes store the packet upon receiving an accept.
      Learns are exchanged, and when a node has received learns from a
      majority of the nodes, the packet will be processed in order and sent
      to each end-host.}
  \label{flow:simple}
\end{figure}

What we have accomplished here is using Paxos for
ordering\index{Paxos!ordering}\index{ordering} the client
requests down to the hosts, so that each host will receive packets in the
same order.  To test that the hosts have received packets in the same order,
we have run a simulation where several clients send packets to them and then
compare their output checksums using the SHA-256 algorithm.

\section{The Final Set of Flow Entries}
\label{chapter:final.flowtable}

In tables \ref{table:complete.match.leader} and
\ref{table:complete.match.slave}, we show the final table of events that the
Paxos leader and slaves will handle, respectively.
%
The tables have been implemented both on the Paxos controllers and the Paxos
switch (Open vSwitch), except for the process queue on the switch, making it
impossible to get results for round-trip times.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|}
    \hline \textbf{Match} & \textbf{Action} \\
    \hline Packet from client & Stamp with PAXOS CLIENT Ethernet type \\
                       & Forwward to leader \\
    \hline Packet from end-host & Forward to client \\
    \hline PAXOS JOIN  & Store MAC address and node id of switch \\
                       & (\textit{Paxos-on-controller only}) \\
    \hline PAXOS CLIENT & Execute on-client \\
                        & Send PAXOS ACCEPT to all nodes with a copy of the
                        packet \\
    \hline PAXOS LEARN & Execute on-learn, forward client packet to hosts  \\
    \hline
  \end{tabular}
  \caption{The final event table for the Paxos leader.}
  \label{table:complete.match.leader}
\end{table}

In the case of the controller, these events have been implemented as
procedure calls, dispatching on Ethernet types signifying different Paxos
messages.  To forward messages on the network, OpenFlow commands were sent
down to the switch.

For the implementation on the switch (Open vSwitch), these tables have been
implemented as flow entries in the flow tables.  Everything except the
processing of the queue were imple,ented, making it impossible to measure
round-trip times.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|l|}
    \hline \textbf{Match} & \textbf{Action} \\
    \hline Packet from client & Forward to leader \\
                       & Optionally, set unique packet ID \\
    \hline Packet from host & Forward to client  \\
    \hline PAXOS JOIN & Store MAC address, node id and leader-flag \\
    \hline PAXOS ACCEPT from leader & Execute program on-accept \\
                                    & Store packet \\
                                    & Send learn to all, if applicable \\
    \hline PAXOS LEARN & Execute program on-learn \\
                       & Run process queue, forwarding to hosts in-order \\
    \hline
  \end{tabular}
  \caption{The event table for the Paxos slaves.}
  \label{table:complete.match.slave}
\end{table}

Some details have been omitted from tables, and we refer to the source code
for further detail (see the appendix, chapter \ref{chapter:compiling}).


