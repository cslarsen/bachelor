\chapter{Analysis}

Here we will look at the performance profile of our various configurations.
Note that we are running everything on a software network simulator, and
therefore these performance tests will only be useful for giving us an
indication of \textit{relative} performances.

Therefore we will first need to establish a baseline for the system as it
is. We should then be able to compare various configurations against it.

\todo{Når ferdig, test med ekte software som MySQL, osv osv.}

For reference, the teste have been run on an Ubuntu GNU/Linux VM from the
Mininet site, loaded with our code and running in VirtualBox on a Mac OS X
laptop.

\section{Baseline --- ICMP ping on L2 learning switch using flow tables}

The most basic setup we can test against is using our L2 learning switch
from chapter \ref{chapter:l2.learning.switch}
\vpageref{chapter:l2.learning.switch}.  This is fundamental to all our
configurations, because they all use it to make sure that packets are routed
correctly.

We will use the topology given in figure \ref{figure:baseline.topology}.

\begin{figure}
  \centering
  \begin{tikzpicture}[
    every node/.style={draw, circle},
    x=0.6cm,
    y=0.6cm]

    % Switches
    \foreach \n in {1,2,3} {
      \pgfmathsetmacro\x{(\n-2)*6}

      % Switch
      \node (S\n) at (\x ,  0) {$S_\n$};

      % Controller
      \node (C\n) at (\x ,  2) {$C_\n$};
      \draw (S\n) -- (C\n);

      % Hosts
      \foreach \h in {1,2,3} {
        \pgfmathsetmacro\pos{(\h - 2)*2}
        \pgfmathtruncatemacro\num{((\n - 1)*3) + int(\h)}

        % Host node
        \node (h\num) at (\x + \pos, -2) {$h_{\num}$};
        \draw (S\n) -- (h\num);
      }
    }

    % Switch links
    \draw (S1) to[out=10,in=170]
               node[below=-0.5cm, draw=none] {$l_1 = 5~ms$} (S2);

    \draw (S2) to[out=10,in=170]
               node[below=-0.5cm, draw=none] {$l_2 = 5~ms$} (S3);

    % Mark traversal path
    \draw [very thick] (h1) -- node[left,draw=none] {$l_0=5~ms$} (S1);
    \draw [very thick] (S1) -- node[left,draw=none] {$l_{C_1} \approx 0~ms$} (C1);
    \draw [very thick] (S1) to[out=10,in=170] (S2);

    \draw [very thick] (S2) -- node[left,draw=none] {$l_{C_2} \approx 0~ms$} (C2);
    \draw [very thick] (S2) to[out=10,in=170] (S3);

    \draw [very thick] (S3) -- node[left,draw=none] {$l_{C_3} \approx 0~ms$} (C3);
    \draw [very thick] (S3) -- node[right,draw=none] {$l_4=5~ms$} (h9);
  \end{tikzpicture}
  \caption{Baseline topology with three switches $S$ and their controllers
    $C$.  The client $c_0$ will send ICMP ping packets to the farthest node
      $h_9$.  The packets will go through three links with a configured
      latency of $5~ms$.}
  \label{figure:baseline.topology}
\end{figure}

Before we present the results, let's look at what we should expect from the
configuration above.  The three links the packets need to cross each have
$5~ms$ latency.  There is some latency in each switch and in each end of the
link ($c_0$ and $h_9$) as well as between and in the switches and their
controllers.

In total, one would expect the \acf{RTT} to be
\begin{gather}
  RTT_{c_0, h_9} = 2\left( \sum l + \sum P_S + \sum P_C \right) + P_{c_0} + P_{h_9} + K
  \label{equation:baseline.rtt}
\end{gather}%
that is, the latencies for all links $l$, processing time $P$ for
$S,C,c_0,h_9$ and a constant $K$ for inherent latency in the software
environment (Mininet, Open vSwitch and VirtualBox).  Equation
\ref{equation:baseline.rtt} is loosely based on
\cite{DBLP:conf/cnsm/PhemiusB13}.
\todo{Refiner formel, den kan ha feil!}

For simplicity, we will set $P_{c_0},~P_{h_9},~l_{C_1},~l_{C_2}~\text{and}~
l_{C_3}$ to zero.\footnote{When flow table entries are installed, one should
see very little traffic going to the controllers, meaning that we can
assume that $l_{C} \to 0$ when the flow tables have been ramped up.
There are still events that are being sent from the switches to the
controllers, though.}
For consistency, we have removed the fall--back link
between $S_1$ and $S_3$ (figure \ref{figure:graph.three.switches}).
We could attempt to measure $K$ by running Mininet with two nodes linked
with zero bandwidth, but we will simply set it to zero as well.

Filling in the known values gives
\begin{gather}
  RTT_{c_0,h_9} = 40~ms + 2\left( \sum P_S + \sum P_C \right)
  \\
  RTT_{c_0,h_9} = 40~ms + 2L~\text{where}~L = \sum P_S + \sum P_C
  \label{equation:expected.baseline.rtt}
\end{gather}

We will now use the BSD \texttt{ping} command to send out packets every
$10~ms$.  The controller will install flow tables with idle and hard timeout
set to one hour.  Remember that there is some inherent noise in the system
as, e.g., ARP--packets will need to be exchanged.  The installed flow
entries will match on as many fields as possible.\todo{Cite kildekode
som vi bruker, flow installation er tatt fra POX sin l2 switch.}
We sent 1000 ping requests with an interval of 1 ms for each packet.
The idle and hard timeout for the flow table entries was set to one hour.

You can run this test yourself by setting up the thesis GNU/Linux VM
by following the instructions in \ref{chapter:install.vm}
\vpageref{chapter:install.vm}. The recommended way is to run Mininet and POX
in separate terminal windows. You are strongly recommended of using the
\texttt{\-{}t} option for ssh, so that you're not leaving behind any
dangling processes if you decide to terminate the session with CTRL+D.

Instructions on how to run this test is given in
\ref{chapter:appendix.baseline.benchmark}
\vpageref{chapter:appendix.baseline.benchmark}.

The summary of ICMP pings are given in table \ref{table:baseline.summary}.

\input{data/pings-summary.tex}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{data/pings.pdf}
  \caption{\acs{RTT} for ICMP PING on the L2 learning switch using flow tables.
           We can clearly see the ramp--up time for the controller as it
           adds flow table entries to have the switch automatically forward
           Ethernet packets. The plot shows the \textit{median} value in red
  and the \textit{mean} in gray.  Because of the ramp--up phase, the mean is a poor
  indicator for the values after warm--up.  As we have a relatively large
  number of samples, the median gives a more realistic picture of the
  situation.  This can clearly be seen in the plot.}
  \label{benchmark:l2.learning.switch.ping}
\end{figure}

The results have been plotted in figure
\ref{benchmark:l2.learning.switch.ping}
\vpageref{benchmark:l2.learning.switch.ping}.
The median RTT was $43.80~ms$, the mean $\mu = 45.87$ with a standard deviation
$\sigma = 16.507~ms$.\footnote{We assume a normal distribution.}
This is well in line with our expectations.  Let's look at equation
\ref{equation:expected.baseline.rtt} again. Filling in the
results, we get\footnote{We'll use the median value, because when you have a
ramp--up and occasional spikes, the median value better reflects the value
in normal mode of operations.}
\begin{gather}
equation:expected.baseline.rtt
  RTT_{c_0,h_9} = 40~ms + 2L = 43.80~ms \\
  L = \sum P_S + \sum P_C = 1.9~ms \\
  \text{but}~P_C \to 0~\text{as the flow table is built, so} \\
  \sum^{3} P_S = P_{S_1} + P_{S_2} + P_{S_3} = 1.9~ms \\
  P_S \approx 0.63~ms
\end{gather}

In other words, the one--way latency per switch is about
0.63~ms. We won't look more into this result, because we are simply
measuring the performance of Open vSwitch and its code that implements
latency control.

However, we now want to run the same test, but without flow tables. This
should place all the load on the controllers.  We do this to create a
bounding range of latency values that we expect our final Paxos system to be
in. We will be using a combination of flow table entries and controller code
to implement Paxos, so the final latency of of each controller--switch pair
should be slightly larger than $1.9~ms$ but also smaller than the next
test.\todo{Skriv bedre, endre rekkefølge}.

\begin{gather}
  \sqrt{\frac{1}{|n]}\sum{n^2}}
  \label{equation:rms}
\end{gather}

was $17.00~ms$.
\todo{Remove outliers and show numbers.}
The minimum and maximum values were $15.30~ms$ and $50.50~ms$.
The 1st quartile was at $16.30~ms$ and its 3rd at $16.75~ms$ and its standard
deviation $\sigma = 2.665~ms$.

\todo{Bruk boxplot() i R når en skal sammenligne. Og hiv inn tall i tabell
  for sammenligning sammen med plot. Plot trenger ikke være gigantisk stor.}

\section{L2 learning switch, key--value store}
\label{chapter:benchmark.l2.kv.noflows}

First we need a baseline.  Here we have a topology of two switches $S_0$ and
$S_1$ with a controller each.  The switches has three hosts, $h_0, \dots, h_5$.
A client $c_0$ is connected to $S_0$. We're running Python
key--value--stores on each host, and $c_0$ will issue a get--request
followed by a put--request to the host $h_5$, which is connected to $S_1$.
We measure the time for each request, divide it
by two and call it latency.\footnote{We basically assume that the packets
take an equal amount of time back and forth, and divide by two to get this
time.}

There are two controllers $C_0$ and $C_1$ for the switches $S_0$ and $S_1$,
respectively.  In this situation, the packets from $c_0$ need to travel
over two switches.

The network is running on Mininet, a software network simulator, on a Linux
virtual machine running on an Mac OS X box.  All the links in Mininet have
been set up with $10 Mbit/s$ bandwidth, $5~ms$ latency and no packet
loss.  These links have been set up with \ac{HTB}
\cite{devera2002hierarchical} enabled, which Open vSwitch\index{Open vSwitch}
 uses for providing rate limiting.

In this first benchmark, the switches will send each packet up to the
controllers.  The controllers implement L2 learning switches and will not
install any flow table entries for rapid dispatch.

On the x--axis is the time in seconds.  On the y--axis is the latency for
get and put requests.

The result can be seen in figure \ref{benchmark:l2.learning.switch.no.flows} 
\vpageref{benchmark:l2.learning.switch.no.flows}.

What is surprising here is that the put--requests have much higher latency
than the get--requests. We don't know why this is.\todo{Finn ut! Er det pga
  pakkene er større? Eller andre grunner?}

\section{L2 learning switch, key--value store, flow entries}

We have the same setup as above, but this time we install flow entries that
will automatically forward the packets.  The flow table entries will match
on as many fields in the packets as possible.

The flow table entries have idle and hard timeouts set to 10 seconds.
In other words, we should be able to see some increased latency every ten
seconds.

% Present both plots together
\begin{figure}
  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{data/data2.eps}
    \caption{L2 learning switch without using flow tables.}
    \label{benchmark:l2.learning.switch.no.flows}
  \end{subfigure}

  \centering
  \begin{subfigure}{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{data/data3.eps}
    \caption{L2 learning switch using flow tables.}
    \label{benchmark:l2.learning.switch.with.flows}
  \end{subfigure}
\end{figure}

The result can be seen in figure \ref{benchmark:l2.learning.switch.with.flows}
\vpageref{benchmark:l2.learning.switch.with.flows}.

Here we can see that the latency has been reduced somewhat.\todo{Hvor mye?
Hva med å plotte over hverandre, hva med å lage average eller root mean
square eller noe sånn?}

We also see some latency spikes around every ten seconds, as expected.

\todo{Plott med flere verdier, istedenfor å vise seconds på x--aksen kan vi
  vise elapsed time, som i at den starter på 0 sekunder.}

These two benchmarks will serve as a baseline to which we will compare our
performance when we enable Paxos on the switches.

Remember that when we run Paxos, we will still have to use the L2 learning
switch for the nodes to be able to communicate.

\todo{Trendlinjner, moving average, fitting osv... må ha det i grafene, må
  også ha litt statistisk analyse av tallene selv osv.}

\section{Three switches, Paxos on controller}

\todo{Få data}

\section{Three switches, Paxos on controller and flow table}

\todo{Få data}

\section{Other solutions}

We chose to use OpenFlow as the basis for our system.
We could just as well have used a networking system that already supported
programmability in some way, for instance the Intel DPDK\todo{Needs
citation}.\todo{Also needs defense.}

\todo{Flytt evt dette ned til improvements--delen}
