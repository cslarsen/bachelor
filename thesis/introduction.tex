\chapter{Introduction}

Around 2008, \textbf{\acf{SDN}}\index{software-defined networking}
\cite{Casado:2005:VNS:1047344.1047383} emerged from research at
Berkeley\index{Berkeley} and
Stanford\index{Stanford} as a way to enable networks to be defined and managed using
software. One model of \acs{SDN} is \textit{OpenFlow}\todo{Cite!}, which
decouples the control plane\index{control plane} from the forwarding
plane\index{forwarding plane}\index{data plane|see{forwarding plane}} in a
switch, moving it out of the physical switch to an external controller node.
It enables one to implement the controller in software.

Although invented quite recently, software-defined networking is already
being heavily used both in academia and industry.  Google\index{Google}, for instance, are
using OpenFlow to ease deployment and increase utilization in their backbone
networks\index{backbone network} \cite{crabbe2012sdn} and Stanford has deployed several
OpenFlow-controlled networks on their university network.

In March 2013, IDC\index{IDC} \todo{finn ut hva det står for} projected that the SDN market would
  reach \${}3.7 billion by 2016, capturing a 35\%{} share of the switching
  market \cite{Kirkpatrick:2013:SN:2500468.2500473}.
\todo{Skriv om denne setningen, den er tatt nesten direkte fra artikkelen!}

OpenFlow is detailed in \vref{chapter:background.openflow}.

\textbf{Paxos}\index{Paxos} \cite{Lamport:1998:PP:279227.279229} is a
family of distributed, fault-tolerant consensus algorithms.  It allows
network nodes to reach \textit{agreement} even in the face of intermittent
network failures.  For example, one can design a database system using Paxos
to make sure that transactions are executed in the same order on all nodes.

Originally published by Leslie Lamport\index{Lamport, Leslie} in 1989, Paxos
has spawned numerous extensions, including cheap Paxos,
\index{cheap Paxos}\index{Paxos!cheap Paxos} fast Paxos
\index{fast Paxos}\index{Paxos!fast Paxos} and Byzantine
\index{Paxos!Byzantine Paxos}\index{Byzantine Paxos}, fault-tolerant variants.

It is discussed in \vref{chapter:background.paxos}.

\textbf{Our aim} is to build an efficient, \textit{Paxos-enabled software defined
network}.  Paxos will be implemented on OpenFlow switches to guarantee that
packets sent to all of their connected nodes are sent in the same order.
These end-hosts can run any networking service and leverage the benefits of
Paxos without needing to handle any details of the algorithm.

\section{Hypothesis}

For simplicity, we will constrain our scope to a few primitives of
classic crash Paxos\index{Paxos!classic crash} in \textit{phase two},
where we have steady-state
flow\index{Paxos!steady-state flow} with no failures\index{Paxos!failure}.

Furthermore, we want to look at opportunities for increasing networking
performance by moving parts of the Paxos from the control
plane\index{control plane} down to the switches' forwarding
plane\index{forwarding plane}.

We will implement this in progressive stages:

\begin{enumerate}
  \item Implement Paxos entirely in controllers connected to an OpenFlow
  switch.

  \item Extend OpenFlow and Open vSwitch so we can execute Paxos in the
  forwarding plane of the switch.

  \item Move parts of the Paxos implementation down to the forwarding plane
  (OpenFlow \textit{flow tables}), achieving a good balance of performance and
  programming maintainability
\end{enumerate}

Our hypothesis is two-fold:

\begin{enumerate}
\item Network nodes can leverage Paxos guarantees \textit{transparently} by
implementing it on the software switches using OpenFlow.
\item We can achieve good relative performance by moving parts of the
implementation from the OpenFlow controllers down to the software switches.
\end{enumerate}

We aim to build a proof-of-concept system backing up these claims.  The
thesis will therefore be a study of \textit{feasibility}.

\section{Overview}

\todo{Flytt dette inn i teksten over, trenger ikke være egen section.}
We discuss the theoretical background needed to understand this thesis in
chapter \ref{chapter:background} \vpageref{chapter:background}.  If you already know \acs{SDN},
OpenFlow and Paxos, you can skip this chapter.

Then we look at what OpenFlow can offer us in chapter
\ref{chapter:details.openflow}
\vpageref{chapter:details.openflow}, detail what Paxos requires for
implementation in chapter \ref{chapter:details.simplified.paxos} 
\vpageref{chapter:details.simplified.paxos} and propose a
design based on this in chapter \ref{chapter:design} \vpageref{chapter:design}.
\todo{Sjekk at linker stemmer, kan gjerne forenkles også + forbedres}

Finally, we .... blabla ... look at benchmarks and conclude in chapter
\ref{chapter:conclusion} \vpageref{chapter:conclusion}.

\section{Scope}

\todo{Fra Hein, skriv om.}
The objective in this thesis is to explore the feasability of implementing a
distributed protocol in the switch.  It is specifically not a goal to
implement a full Paxos implementation that captures all the possible corner
cases of Paxos to tolerate failures.
%

% Også fra Hein
While Paxos is specifically about fault-tolerance, we are only intereste in
understanding the capabilities and feasability of implementing Paxos in the
switch.

We will only look at a simplified version of Paxos in which we only
implement accept and learn messages. We ignore liveness checking such as
heartbeats. We ignore implementing the expanded OpenFlow features in the
network protocol and controller. We only look at steady state Paxos.
We assume switches are co-located. etc etc etc.

We also ignore the fact that if a switch goes down and back up again, it
will need to rejoin the Paxos network and its end-hosts need to synchronize
state\index{synchronization} (or just copy the state from a host on another switch).

\section{Topology}

A premise for for implementing Paxos in the switch is that we also deploy
multiple switches to provide the necessary resilience to failures.
%
Thus we need to provision a switch topology.

In this section, we will present how we plan to achieve this. 
%
Starting with a single switch, we show how it can use a controller to
shuttle packets between clients and services running on end-hosts (figure
\ref{figure:graph.single.switch}).
%
By linking three such switches together, one will typically want to install
Paxos middleware on each host as a way to provide service replication, and
thus, fault-tolerance (figure \ref{figure:paxos.on.servers}).
%
We then propose that by moving Paxos from the hosts to the
switches, one can offer the guarantees of Paxos---\textit{transparently} for
\textit{any} service (figure \ref{figure:paxos.on.switches}).

Let us start with the simplest case: One switch.\index{Paxos!topology}

In figure
\vref{figure:graph.single.switch}, we have a switch $S_1$ with several
input and output networking ports.  On one of these, the switch
will receive packets from a number of clients, $c_1$, $c_2$, $\dots$, $c_n$.
%
It also has an OpenFlow controller $C_1$ and three end-hosts $h_1$, $h_2$ and
$h_3$.

Note that the actual location of the clients is irrelevant to our current
discussion.  Our focus is on the possible merits of placing Paxos on the
switch, not on how clients are able to reach the services. We will therefore
\textit{assume} that we are able to communicate with a set set of clients on
designated ports.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[
      every node/.style={draw, circle, minimum width=0.75cm},
      x=0.7cm,
      y=0.7cm]
    \node (c1)    at (-3,  2) {$c_1$};
    \node (c2)    at (-5,  2) {$c_2$};
    \node [dashed] (cn) at (-7, 2) {$c_n$};

    \node (ctrl1) at ( 0,  2) {$C_1$};
    \node (S1)    at ( 0,  0) {$S_1$};
    \node (h1)    at (-2, -2) {$h_1$};
    \node (h2)    at ( 0, -2) {$h_2$};
    \node (h3)    at ( 2, -2) {$h_3$};

    \draw [thick] (c1) to[out=310,in=140] (S1);
    \draw [thick] (c2) to[out=310,in=140] (S1);
    \draw [dashed] (cn) to[out=310,in=140] (S1);

    \draw (S1) -- (ctrl1);
    \draw (S1) -- (h1);
    \draw (S1) -- (h2);
    \draw (S1) -- (h3);

  \end{tikzpicture}
  \caption{A single switch $S_1$ with its controller $C_1$, connected
    to end-hosts $h_1$, $h_2$, $h_3$ and clients $c_1$, $c_2$, $\dots$, $c_n$
    through ports.}
  \label{figure:graph.single.switch}
\end{figure}

As the switch receives packets, it will try to match packets with
patterns in the \textit{flow table}.
%
The flow table contains a set of patterns and associated actions to
perform in the case of a match.  The actions can be to forward the packet,
drop it and so on.
%
Dispatching on this level is very fast.  The switch can potentially reuse
well-established data structures and algorithms for looking up rules and
acting on them.  Hardware switches can even perform at line-speed.

In case a packet does not match any flow table rules---or
\textit{flows}---its headers will be forwarded to the OpenFlow controller
$C_1$.
%
The controller is a component that communicates with the switch using the
OpenFlow protocol on a secure channel.
%
It can issue commands to the switch, telling it to install flows or perform
immediate actions on the packet.

When the network starts up, the flow tables will be empty.
Thus, all packets are by default forwarded to the controller.
%
During this phase, OpenFlow controllers usually start off by explicitly
instructing the switch what to do with each packet.
At the same time, it will start building an internal map of the network,
noting which ports it has seen Ethernet and IP addresses on.
%
As this map becomes more complete, it can begin installing flows on the
switch.
%
For instance, if it knows the port numbers for a pair of addresses, it can
add a flow that automatically forwards packets to the appropriate ports when
the two addresses are involved.
%
In the reverse case, it can just broadcast packets to all ports, knowing
that only designated receivers will act on them.
%
Such behaviour, a \textit{learning switch}, is explained in detail in
chapter \vref{chapter:l2:learning.switch}.

Along with each flow entry is an associated set of timeouts.  When a flow
times out, it is removed from the flow table.  This will start a new round
of trips up to the controller, who can then install new flows---or updated
ones, in case the topology has changed in the meantime.
%
This serves several purposes.  First of all, it makes sure that flow tables
will not fill up.  Flows and packets are transient by nature, and the
controller has to step in from time to time to update the flow rules.
%
Secondly, this mode of operation does not require any up-front
configuration.
%
Properly behaving controllers will be designed to handle ever-changing
situations.  In the beginning, they will do all the heavy-lifting.
As it learns, it will offload work to the switch---but it will never assume
that it has a complete model of the network.
%
Just plug it in, and it will work.

In figure \vref{figure:graph.single.switch}, the switch has three additional
nodes---$h_1$, $h_2$ and $h_3$.  In this thesis, these will refer to
\textit{end-hosts} running some kind of service: Web-servers, databases,
lock-servers and so on.  These are the type of services that we want to
replicate.

\todo{Ny tekst over, må nok flyttes, gammel tekst under, må nok slettes.}

The \textit{end-hosts}\index{end-host} are nodes connected to a single
switch and running services such as key-value stores\index{key-value store},
lock-servers\index{lock-server}, logging servers\index{log server},
databases\index{database}, and so on.

Here is the situation with three switches---the minimum number of nodes we
need in a fault-tolerant Paxos system.  We have also indicated fail-over
links, shown as dashed lines in the figures.  These are in case one of the
swithes go down.  Of course, the clients should also have fail-over links,
but this is not interesting for this thesis: One can imagine the clients
being on a network with several fail-over links down to the switches.
Coming up with topologies like that is a study in itself, and we will simply
assume that, somehow, client packets are forwarded to our switches.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[every node/.style={draw, circle},x=0.7cm,y=0.7cm]
    \foreach \n in {1,2,3} {
      \pgfmathsetmacro\x{(\n-2)*6}
      \node (c\n)    at (\x - 2,  2) {$c_\n$};
      \node (ctrl\n) at (\x ,  2) {$C_\n$};
      \node (S\n)    at (\x ,  0) {$S_\n$};

      \draw (c\n) to[out=305,in=125] (S\n);
      \draw (S\n) -- (ctrl\n);

      \foreach \h in {1,2,3} {
        \pgfmathsetmacro\pos{(\h - 2)*2}
        \pgfmathtruncatemacro\num{((\n - 1)*3) + int(\h)}
        \node (h\num) at (\x + \pos, -2) {$h_{\num}$};
        \draw (S\n) -- (h\num);
      }

    }

    % Links between switches
    \draw (S1) to[out=-10,in=190] (S2);
    \draw (S2) to[out=-10,in=190] (S3);
    \draw [dashed] (S1) to[out=-15,in=195] (S3);

  \end{tikzpicture}
  \caption{Three switches $S_1, S_2, S_3$ with controllers $C_1, C_2, C_3$ acting as Paxos nodes.
           The dashed line between $S_1$ and $S_3$ is a possible \textit{fail-over link}.}
  \label{figure:graph.three.switches}
\end{figure}

The point is that these services will be mirrored by the use of our
Paxos-enabled switches.  For our purposes, we will assume that the services
on these hosts are \textit{deterministic}\index{deterministic}
in the sense that the input uniquely determine the
state of the service after being processed---if two hosts running the
same service receive the exact same packet, their state will be
identical after having processed it.  This is a prerequisite for our
system.  The OpenFlow switches, running Paxos, will only make sure that
packets are delivered in the \textit{same order} to the end-hosts.

\section{Viability}

Why would such a system be useful? Consider the situation of implementing
Paxos in code on some servers, running services such as key-value
stores\index{key-value store}, etc.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[every node/.style={draw, circle},x=0.7cm,y=0.7cm]

    % For each switch ...
    \foreach \n in {1,2,3} {
      \pgfmathsetmacro\x{(\n-2)*6}
      \node (ctrl\n) at (\x ,  2) {$C_\n$};
      \node (S\n)    at (\x ,  0) {$S_\n$};

      \draw (S\n) -- (ctrl\n);

      % For each host ...
      \foreach \h in {1,2,3} {
        \pgfmathsetmacro\pos{(\h - 2)*2}
        \pgfmathtruncatemacro\num{((\n - 1)*3) + int(\h)}

        % Host node
        \node (h\num) at (\x + \pos, -2) {$h_{\num}$};
        \draw (S\n) -- (h\num);

        % Paxos node
        \node [very thick] (P\num) at (\x + \pos, -3.75) {$P$};
        \draw [dashed] (h\num) -- (P\num);
      }
    }

    % Links between switches
    \draw (S1) to[out=-10,in=190] (S2);
    \draw (S2) to[out=-10,in=190] (S3);
    \draw [dashed] (S1) to[out=-15,in=195] (S3);

  \end{tikzpicture}
  \caption{Support for Paxos on the servers $h_1, \dots, h_3$ requires a
    copy of the Paxos code $P$ on each server.}
  \label{figure:paxos.on.servers}
\end{figure}

Here, each server needs to have an implementation of Paxos in code (shown as
$P$ in figure \ref{figure:paxos.on.servers}).  It means the software
developer has to specifically add support for Paxos when designing the
server code, tailoring it for the particular service.  All Paxos handling
must be done at a high networking layer\index{networking layers}---most
likely in the application layer\index{application layer} at the very top.

Now consider the situation where the switch provides Paxos
capabilities\index{Paxos!on switch} (figure \ref{figure:paxos.on.switches}).

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[every node/.style={draw, circle},x=0.7cm,y=0.7cm]

    % For each switch ...
    \foreach \n in {1,2,3} {
      \pgfmathsetmacro\x{(\n-2)*6}
      \node (ctrl\n) at (\x ,  2) {$C_\n$};
      \node (S\n)    at (\x ,  0) {$S_\n$};

      \draw (S\n) -- (ctrl\n);

      % Paxos node
      \node [very thick] (P\n) at (\x + 2, 1) {$P$};
      \draw [dashed] (S\n) -- (P\n);
      \draw [dashed] (ctrl\n) -- (P\n);

      % For each host ...
      \foreach \h in {1,2,3} {
        \pgfmathsetmacro\pos{(\h - 2)*2}
        \pgfmathtruncatemacro\num{((\n - 1)*3) + int(\h)}

        % Host node
        \node (h\num) at (\x + \pos, -2) {$h_{\num}$};
        \draw (S\n) -- (h\num);
      }
    }

    % Links between switches
    \draw (S1) to[out=-10,in=190] (S2);
    \draw (S2) to[out=-10,in=190] (S3);
    \draw (S1) to[out=-15,in=195] (S3);

  \end{tikzpicture}
  \caption{Paxos ($P$) on the switches $S_1, S_2, S_3$ mitigates the need for special code on the servers.}
  \label{figure:paxos.on.switches}
\end{figure}

In figure \ref{figure:paxos.on.switches}, the switches themselves (and their
controllers\index{Paxos!controller}) enable
support for Paxos.\footnote{We have moved the servers to several switches
to indicate a distributed nature between the switches.
Paxos on a single switch would not be very useful, as that would be a single
point of failure and---after all---be the sole decision point for message
ordering.}
This should let the servers be oblivious of the fact that Paxos is used to
enable ordering of packet arrival to them.

Besides, Paxos is now run at a much lower networking level\index{networking
layers} and at the point where switching is done---there will be less hops
for each packet.

Of course, there are pros and cons for each scenario.
When implementing Paxos, one can often take advantage of
the particular way each server operates. Sometimes one actually
\textit{needs} to know this to implement Paxos.  Therefore, an
implementation of Paxos in each server's code base would be beneficial.

Also, it means that we need to run code on the switches themselves. This
gives rise to a wide range of non-functional requirements for the code.
For instance, code that runs for too long must be
preempted\index{preemption} to let the switch smoothly handle other
requests. Not doing so may result in packet loss, high latency or
worse---complete incapacity to serve data. Besides, switches do not normally
have hardware capable of running \textit{generic} software fast. They
usually have highly optimized hardware to do some of the heavy-lifting.

On the other hand, in our scenario, we can potentially add support for
message ordering through Paxos on servers that does not already support it.
The kinds of services that this would work on is for systems that are
deterministic in nature: The same input (or client packet) on any server
would produce the same internal state and output.
%
But there is a big class of software that has this behaviour:  Key-value
stores, logging servers, relational databases and so on.\footnote{But not
lock servers, because a lock can only be held by one server at a time.}

The only thing our model does not cover is the situation where a switch goes
completely down.
In that case, each connected server should start
synchronizing their internal state with any of the others that have been up.
\todo{Mangler jo også leader election og failures. Må få
  fram at jeg mener at KONSEPTET støtter mange ting, men ikke vår
    IMPLEMENTASJON.}

All in all, we believe this is a viable experiment with practical benefits.
Indeed, using a software-based approach for switches lets us test our model
on \textit{any} existing piece of server software.

Moreover, we also intend to show how one can optimize the performance of
this model further, by moving parts of the Paxos implementation down into
the switches' flow tables.  This should have a big impact, because Paxos
handling is done at a lower networking layer and closer to the central network
components than what would be the case with Paxos in the server's code.

\todo{Føler vi trenger flere, sterkere argumenter.}
