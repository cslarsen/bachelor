\chapter{Background}
\label{chapter:background}

In chapter \ref{chapter:introduction} we stated our thesis goal of offering
Paxos at the networking-level and building a distributed replication system
on top of it.
%
Here, we will present the background material for algorithms and technology
we will use in the rest of this thesis.

\section{OpenFlow}
\label{chapter:openflow.background}

As mentioned in chapter \ref{chapter:introduction}, the core idea of
\acs{SDN} is to dissociate the controller from the switch, enabling software
developers to ``program the network''.  This is illustrated in figures
\ref{figure:coupling.planes} and \vref{figure:decoupling.planes}.

OpenFlow \cite{McKeown:2008:OEI:1355734.1355746,openflow-1.0} is one of
several ways to enable \acf{SDN}
%
It is a specification of a messaging protocol for the interaction between
a switch and controller, detailing what kind of actions that can be
performed by each of these components.
%
Examples of \ac{SDN} solutions predating OpenFlow are SANE
\cite{Casado:2006:SPA:1267336.1267346}, Ethane
\cite{Casado:2007:ETC:1282427.1282382}, 4D
\cite{Greenberg:2005:CSA:1096536.1096541}.

In OpenFlow, the controller is a piece of software can inspect packets and
send commands down to the switch.  It can instruct the switch perform actions like
forwarding a packet to an output port going to another switch, changing
header fields, drop it and so on.
%
Additionally, the switch can be set up to perform such actions by itself,
using a \textit{flow table}.  This is what we have previously referred to as
the \textit{forwarding plane}, and as highly performant in comparison to
handling each individual packet in the controller.

OpenFlow controllers can be written in any programming language that offers
an OpenFlow framework.  While the OpenFlow switches are usually also
implemented in software, several vendors now ship hardware switches that
support the OpenFlow protocol.

\subsection{The Flow Table}
\label{chapter:theory.flow.table}

The aforementioned flow table consists of several flow
entries---colloquially called \textit{flows}.
%
These contain rules for matching packets and actions to perform when there
is a match.
%
In addition there are several counters used for collecting various
statistical metrics (e.g., how many times a flow has been executed) and
timeouts that dictate how long an entry will exist in the table.

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[every node/.style={draw, circle},
                        node distance=2.5cm]
      \node [rectangle,
             rounded corners,
             minimum width=3cm,
             minimum height=1cm] (Sbig) at (0,0) {};

      \node [draw=none] (S) at (0,0) {$S$};
      \node [right=0.25cm of S] (C) {$C$};
      \node [left=0.25cm of S] (F) {$F$};

      % Add invisible node from figure B so that they align vertically
      \node [draw=none] (vspacer) [above of=S] {};

      \node (h1) [dashed, below left of=S] {};
      \node (h2) [dashed, below of=S] {};
      \node (h3) [dashed, below right of=S] {};

      \draw [dashed] (Sbig) -- (h1);
      \draw [dashed] (Sbig) -- (h2);
      \draw [dashed] (Sbig) -- (h3);

    \end{tikzpicture}
    \caption{A typical switch $S$ combines the control and
      forwarding planes ($C$ and $F$) on the same device.
        The control plane
        $C$ is usually locked down by the vendor and inaccessible to users.}
    \label{figure:coupling.planes}
  \end{subfigure}%
  \hspace*{0.1\textwidth}%
  \begin{subfigure}[t]{0.45\textwidth}
    \centering
    \begin{tikzpicture}[every node/.style={draw, circle},
                        node distance=2.5cm]

      \node [rectangle,
             rounded corners,
             minimum width=3cm,
             minimum height=1cm] (S) {$S$};

      \node (T) [above of=S] {$C$};
      \node (h1) [dashed, below left of=S] {};
      \node (h2) [dashed, below of=S] {};
      \node (h3) [dashed, below right of=S] {};

      \node (FlowTable) [left=-0.9cm of S] {$F$};

      \draw (T) -- (S);
      \draw [dashed] (S) -- (h1);
      \draw [dashed] (S) -- (h2);
      \draw [dashed] (S) -- (h3);

    \end{tikzpicture}
    \caption{\ac{SDN} decouples the control plane $C$ from the forwarding
      plane $F$ by moving it out of the switch $S$ to an external network
        node.  The \textit{OpenFlow protocol} enables communication between
        $S$ and $C$, making it possible to implement $C$ in software on an
        ordinary computer. In OpenFlow, the connection between $S$ and $C$ is encrypted.}
    \label{figure:decoupling.planes}
  \end{subfigure}
\end{figure}

When a switch receives a packet, it will try to match it with entries in
the flow table (table \vref{openflow.flow.entry.spec}).
%
Each flow contains a \textit{matching pattern}\index{OpenFlow!matching} and
a set of actions\index{OpenFlow!actions}\index{flow table} to perform in
case there is a match.
%
The actions can be to rewrite a header field, forward the
packet to a port, drop it and so on.
%
If a packet does not match any flows, the switch will forward a buffer ID
and packet headers to the controller $C_1$ on a secure channel.

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|}
    \hline \textbf{Header fields} &
           \textbf{Counters} &
           \textbf{Actions} \\
    \hline \dots & \dots & \dots \\
  \end{tabular}

  \caption{The OpenFlow 1.0 flow table.}
  \label{openflow.flow.entry.spec}
\end{table}

The controller can then decide what to do with the packet.  Using the
OpenFlow protocol, it can issue immediate actions to the switch or install
flows, so the switch can operate on its own.

The flow tables are initially empty, meaning that all packets are by default
sent to the controller.  During this phase, the controller will
explicitly handle every packet.  At the same time, it will incrementally
build up an internal map of the network.  As the map forms, it can start
installing flows in the switch.

For instance, if a controller learns the port numbers for a pair of
addresses, it can instruct the switch to automatically forward packets to
their appropriate output ports when they communicate.
%
If a packet's destination port is unknown, the controller can
\textit{rebroadcast}\index{rebroadcasting} it out on all ports except where
it came from, knowing that only designated receivers will accept the packet.
%
What we have described here is a \textit{learning switch}, and is explained
in detail in chapter \vref{chapter:l2.learning.switch}.

Along with each flow entry is an associated set of timeouts.  Flows are
removed from the flow table when they time out.
%
This serves several purposes.  First of all, it makes sure that flow tables
do not fill up quickly.  Secondly, because flows---and packets---are
transient by nature, controllers will be given the chance to update rules
based on changes in the network.
%
Finally, this mode of operation adheres to the principle of autonomous
operation\index{autonomous operation} commonly seen in networking devices.

Well-designed controllers should not need elaborate configuration to work.
So while they initially do all the heavy-lifting by themselves, they will
offload work to the switches, who can then dispatch packets very quickly.

\subsection{Applications of OpenFlow}

We would like to briefly mention a few real-world uses of OpenFlow.

Researchers at Stanford\index{Stanford}
built an OpenFlow network that was able to migrate a virtual video game
server \ac{VM} from California to Japan---while it was running, without
interruption, using the \textit{same} IP-address
\cite{erickson2008demonstration} \cite{kobayashi2013maturing}.

Google are using OpenFlow in their backbone network to increase utilization
\cite{crabbe2012sdn} and is an official member of the governing institution
of OpenFlow, the \ac{ONF}.

The Open Networking Foundation released the first specification of OpenFlow
in 2009 and continually publish point versions, errata and new versions
\cite{openflow-1.0,openflow-1.0.1,openflow-1.0.2,openflow-1.1,openflow-1.2,openflow-1.3,openflow-1.4}.
The most recent one, at the time of writing, is version
1.4\index{OpenFLow!versions}.

\section{Mininet, POX and Open vSwitch}
\label{chapter:mininet}

Mininet \cite{github:mininet} is an open source network simulator that
supports OpenFlow.
%
Using the Python programming language, one can deploy virtual networks on a
laptop, configuring link speeds, percentage of packet loss and so on.
%
Controllers are written using the POX \cite{github:pox} OpenFlow framework.

At the bottom of all this is the Open vSwitch program \cite{github:ovs}.
It is a powerful software switch used by many cloud providers.
It supports most of the OpenFlow functionality, and runs both as a Linux
kernel module and in user space.
%
Open vSwitch is written by many of contributors of the OpenFlow
specifications, and the three projects share close ties.

Together, they form a powerful combination of software that makes it easy
to experiment with networking technology.  We have used them for our thesis
implementation.

\section{Paxos}
\label{chapter:background.paxos}

Paxos \cite{Lam01, Lamport:1998:PP:279227.279229} is a family of
fault-tolerant, distributed consensus algorithms, allowing nodes to reach
agreement in the face of intermittent failures.
%
Originally published by Leslie Lamport\index{Lamport, Leslie} in 1989, Paxos
has spawned numerous extensions, including cheap Paxos,
\index{cheap Paxos}\index{Paxos!cheap Paxos} fast Paxos
\index{fast Paxos}\index{Paxos!fast Paxos} and Byzantine
\index{Paxos!Byzantine Paxos}\index{Byzantine Paxos}, fault-tolerant variants.

We will not give a full account of the Paxos algorithm here, but will
mention the main parts that are relevant for this thesis.  For reference, we
will hereafter keep to the description given in \cite{Lam01}.

Paxos consist of two main phases: \textit{Phase one} and \textit{phase 2}.
The first phase consists of choosing a leader among all the Paxos nodes.
This phase continues until a leader has been chosen by the majority.
As we will only focus on phase two, we will not discuss this part further.

Phase two, also called \textit{steady-phase}, is where \textit{message
ordering} takes place.   We say that we want to reach \textit{consensus} on
some \textit{value}.  A \textit{value} can be anything we want a majority of
the nodes to agree on.  For our purposes, it means we want to determine
which packet to next process (i.e., we want to determine the
    \textit{order} in which to process packets).  Paxos nodes will send
Paxos messages to each other, and they contain various parameters.  We will
return to these in section \vref{section:paxos.consensus.algorithm}.

The \textit{Paxos leader} from the first phase will receive a message
somehow---in our case, it will be a client packet that we wish to distribute
and process in-order---send out an \textit{accept} message containing a
\textit{value} to be agreed upon---the value can for example be a packet or
a reference to a packet, for example---and sending out an \textit{accept}
message to its \textit{Paxos slaves}.  Only the leader sends out accept
messages.

When a slave received an accept, it will first see if this is a message that
came from its current leader.  If so, and if it has not seen this message
before, it will send a \textit{learn} message to \textit{all} Paxos nodes,
including itself.

Upon receiving a learn-message, a node will first check if it belongs in the
current \textit{round}---meaning that the message belongs in the current
round with the given leader.  In other words, if the round number contained
in the learn message is less than the node's current round, this was a
message from a previous point in time in which there was
\textit{possibly} another leader.  If the learn has not been seen before, it
will record how many unique learns it has seen.
%
Whenever a node has received learns from a majority of other nodes, it will
start to process a queue of messages in the order specified by the message
parameters.

We will refrain ourselves from discussing \textit{why} the algorithm works,
even in the face of failures.  For such details, we refer to \cite{Lam01,Insane.Paxos}.

This concludes our very brief account of Paxos, and we will return to it in
section \ref{section:paxos.consensus.algorithm}.
